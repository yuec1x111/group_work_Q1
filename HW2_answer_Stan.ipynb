{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spCA63QihtwO"
   },
   "source": [
    "# 1. Naive Bayes Algorithm and Prediction\n",
    "\n",
    "You just joined a small real-estate analytics team at **HomeWise**, a startup that helps mortgage officers quickly qualify leads. One Monday morning, the product manager rushes in with coffee and a problem:\n",
    "\n",
    "> “We get hundreds of loan inquiries every week. We can’t call everyone so, can we quickly flag which applicants are likely to buy a house so the loan officers can prioritize them? We only have a few simple features from the application form. Can you help solving this problem?”\n",
    "\n",
    "We use the following features:\n",
    "\n",
    "- **Age**: { ≤30, 30–40, >40 }  \n",
    "- **Income**: { Low, Average, High }  \n",
    "- **Student**: { Yes, No }  \n",
    "- **Credit Rating**: { Fair, Good }\n",
    "\n",
    "Your task is to build a simple **Naive Bayes classifier** to predict whether an applicant will buy a house, using only four features that are easy to collect during the online application.\n",
    "\n",
    "The training data (past applicants) is given in the table below:\n",
    "\n",
    "| ID | Age   | Income  | Student | Credit Rating | Buy House |\n",
    "|----|-------|---------|---------|---------------|-----------|\n",
    "| 1  | ≤30   | Average | No      | Fair          | No        |\n",
    "| 2  | ≤30   | High    | Yes     | Good          | No        |\n",
    "| 3  | 30–40 | High    | No      | Fair          | Yes       |\n",
    "| 4  | >40   | Low     | No      | Fair          | Yes       |\n",
    "| 5  | >40   | Low     | Yes     | Good          | Yes       |\n",
    "| 6  | >40   | Average | Yes     | Good          | No        |\n",
    "| 7  | 30–40 | Low     | Yes     | Good          | Yes       |\n",
    "| 8  | ≤30   | Average | No      | Fair          | No        |\n",
    "| 9  | ≤30   | Low     | No      | Fair          | Yes       |\n",
    "| 10 | >40   | Average | Yes     | Fair          | Yes       |\n",
    "| 11 | ≤30   | Average | Yes     | Good          | Yes       |\n",
    "| 12 | 30–40 | High    | No      | Good          | Yes       |\n",
    "| 13 | 30–40 | Average | Yes     | Fair          | Yes       |\n",
    "| 14 | >40   | Average | No      | Good          | No        |\n",
    "\n",
    "### Questions\n",
    "\n",
    "**1a.** What is the probability $$P(\\text{Income = High} \\mid \\text{Buy House = Yes})$$  Round your answer to **3 decimals**.\n",
    "\n",
    "- [ ] *Place your answer here*\n",
    "\n",
    "\n",
    "**1b.** What is the probability  \n",
    "\n",
    "$$\n",
    "P(30 < \\text{Age} < 40 \\mid \\text{Buy House = No})\n",
    "$$  \n",
    "\n",
    "when we apply **Laplace smoothing with α = 1**? Round your answer to **3 decimals**.\n",
    "\n",
    "- [ ] *Place your answer here*\n",
    "\n",
    "\n",
    "**1c.** In the Naive Bayes classifier, compute:  \n",
    "\n",
    "$$\n",
    "P(\\text{Age > 40, Income = High, Student = No, Credit Rating = Good} \\mid \\text{Buy House = Yes})\n",
    "$$  \n",
    "\n",
    "Round your answer to **3 decimals**.\n",
    "\n",
    "- [ ] *Place your answer here*\n",
    "\n",
    "\n",
    "**1d.** Predict whether the following person will buy a house or not using the Naive Bayes classifier:  \n",
    "\n",
    "- Age = ≤30  \n",
    "- Income = Average  \n",
    "- Student = Yes  \n",
    "- Credit Rating = Fair  \n",
    "\n",
    "Compare both class probabilities and state your prediction:  \n",
    "\n",
    "$$\n",
    "P(\\text{Buy House = No} \\mid \\text{features}) \\quad \\text{vs} \\quad P(\\text{Buy House = Yes} \\mid \\text{features})\n",
    "$$  \n",
    "\n",
    "- [ ] *Place your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MT5NVIl9sOUc"
   },
   "source": [
    "# 2. Decision Trees - Best Split\n",
    "\n",
    "After showing some promise with the **Naive Bayes model**, your manager at **HomeWise** wants to try another approach:  \n",
    "\n",
    "> “Naive Bayes was fast, but I want something we can *explain* to our sales team.  \n",
    "> Can we build a decision-making flowchart that says things like:  \n",
    "> *‘If income is high and credit is good, then likely not a buyer’*?  \n",
    "> That way, our agents can follow the logic step by step.”\n",
    "\n",
    "This is exactly what a **Decision Tree classifier** does.  \n",
    "\n",
    "Decision trees use **information gain** to decide the “best question” (or split) at each step.  \n",
    "By asking the most informative questions first, the tree becomes both **accurate** and **interpretable**.  \n",
    "\n",
    "We’ll build a decision tree to predict whether a person will buy a house based on four features:  \n",
    "\n",
    "- **Age**: { ≤30, 30–40, >40 }  \n",
    "- **Income**: { Low, Average, High }  \n",
    "- **Student**: { Yes, No }  \n",
    "- **Credit Rating**: { Fair, Good }  \n",
    "\n",
    "The training data is the same as in the previous task, summarized in the table below:\n",
    "\n",
    "\n",
    "\n",
    "| ID | Age   | Income  | Student | Credit Rating | Buy House |\n",
    "|----|-------|---------|---------|---------------|-----------|\n",
    "| 1  | ≤30   | Average | No      | Fair          | No        |\n",
    "| 2  | ≤30   | High    | Yes     | Good          | No        |\n",
    "| 3  | 30–40 | High    | No      | Fair          | Yes       |\n",
    "| 4  | >40   | Low     | No      | Fair          | Yes       |\n",
    "| 5  | >40   | Low     | Yes     | Good          | Yes       |\n",
    "| 6  | >40   | Average | Yes     | Good          | No        |\n",
    "| 7  | 30–40 | Low     | Yes     | Good          | Yes       |\n",
    "| 8  | ≤30   | Average | No      | Fair          | No        |\n",
    "| 9  | ≤30   | Low     | No      | Fair          | Yes       |\n",
    "| 10 | >40   | Average | Yes     | Fair          | Yes       |\n",
    "| 11 | ≤30   | Average | Yes     | Good          | Yes       |\n",
    "| 12 | 30–40 | High    | No      | Good          | Yes       |\n",
    "| 13 | 30–40 | Average | Yes     | Fair          | Yes       |\n",
    "| 14 | >40   | Average | No      | Good          | No        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B60Y-aPttyAc"
   },
   "source": [
    "**2a.** What is the entropy of the target (i.e. the entropy of the two classes)? Round your answer to three decimals after the point.\n",
    "- [0.94] *Place your answer here*\n",
    "\n",
    "**2b.** What is the information gain if we choose the variable Student as the first split of the tree (i.e. the root node)?\n",
    "- [0.016] *Place your answer here*\n",
    "\n",
    "**2c.** This decision tree predicts Buy House = Yes to all records with Age ∈ (30, 40).\n",
    "\n",
    "- [ ] True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSCUyQrzpoFA"
   },
   "source": [
    "# 3. Netflix Recommender System\n",
    "\n",
    "Imagine you’re working as a data scientist at **Netflix**. Your boss comes to you with a challenge:\n",
    "\n",
    "> “We want to recommend movies to users, but here’s the catch: most people don’t rate many movies!  \n",
    "> Out of thousands of movies, a user may only rate 5 or 10.  \n",
    "> Can we still figure out what they’d like next?”\n",
    "\n",
    "This is exactly the **matrix completion problem**.\n",
    "\n",
    "We represent movie rating data in a matrix $D$:\n",
    "\n",
    "- Each row $i$ represents a user\n",
    "- Each column $k$ represents a movie\n",
    "- Each entry $D(i,k)$ is the rating user $i$ gave to movie $k$\n",
    "- If user $i$ hasn't rated movie $k$, we'll represent this as a 0 in that position\n",
    "\n",
    "Consider this example with 3 users and 4 movies:\n",
    "\n",
    "```text\n",
    "D = [5  0  3  0]\n",
    "    [0  4  0  2]\n",
    "    [3  0  0  5]\n",
    "```\n",
    "\n",
    "The key insight is that we can approximate this sparse matrix $D$ as the product of two smaller matrices:\n",
    "$$\n",
    "D \\approx Y X^T\n",
    "$$\n",
    "Where:\n",
    "\n",
    "- $Y$ is an $n×r$ matrix ($n$ = number of users, $r$ = number of \"latent factors\")\n",
    "- $X$ is a $d×r$ matrix ($d$ = number of movies, $r$ = number of \"latent factors\")\n",
    "\n",
    "Intuitively:\n",
    "\n",
    "- Each row of $Y$ represents a user's preferences across $r$ hidden features\n",
    "- Each row of $X$ represents how much each movie exhibits those $r$ features\n",
    "\n",
    "The dot product of a user's preferences and a movie's features gives us a predicted rating for each combination of user and movie (including the ones without a rating). The goal then is to minimize the approximation error between $D$ and $Y X^T$ only for the observed entries in the rating matrix, which gives us the optimization objective:\n",
    "\n",
    "$$\n",
    "\\min_{X,Y} \\|D - O \\circ (YX^\\top)\\|^2 = \\sum_{(i,k) \\in O} (D_{ik} - Y_{i\\cdot}X_{k\\cdot}^\\top)^2 \\quad \\text{s.t. } X \\in \\mathbb{R}^{d \\times r}, Y \\in \\mathbb{R}^{n \\times r}\n",
    "$$\n",
    "\n",
    "$O \\in \\{0,1\\}^{n \\times d}$ is the indicator matrix for observed entries where $O \\subseteq \\{1,\\ldots,n\\} \\times \\{1,\\ldots,d\\}$ contains indices of observed entries, and $D \\circ O = D$ (missing entries are represented as zeros), with $\\circ$ representing Hadamard (element-wise) multiplication.\n",
    "\n",
    "Since trying to solve this optimization problem for all elements of $X$ and $Y$ at the same time is very complex we will use a different approach: \"Block coordinate descent\". The key idea of this approach is to fix all variables except one \"block\" (in this case, one row of $X$ or $Y$), find the optimal value for that block, move to the next block, and repeat to cycle through all blocks until convergence. By making use of FONC (First Order Necessary Conditions) we can guarantee that we obtain the global minimizer of each block at each step.\n",
    "\n",
    "Doing this requires us to obtain the partial derivative of the block with respect to the objective function, and then setting that to zero to obtain the parameters for the (partial) global minimum.\n",
    "\n",
    "First, the partial gradient with respect to $X_{ks}$ can be expressed as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial X_{ks}} \\sum_{(i,l) \\in O} (D_{il} - Y_{i\\cdot}X_{l\\cdot}^\\top)^2 = -2(D_{\\cdot k} - YX_{k\\cdot}^\\top)^\\top \\text{diag}(O_{\\cdot k})Y_{\\cdot s}\n",
    "$$\n",
    "\n",
    "where $\\text{diag}(O_{\\cdot k}) = O_{Xk}$ is the diagonal matrix of observed entries for feature $k$. Using this, we can use the previous derivative of a single entry to derive the derivative of an entire row:\n",
    "\n",
    "$$\n",
    "\\nabla_{X_k} \\sum_{(i, l) \\in O} (D_{il} - Y_{i \\cdot} X_{l}^\\top)^2 = -2 (D_{\\cdot k }^\\top - X_{k \\cdot} Y^\\top) O_{Xk} Y\n",
    "$$\n",
    "\n",
    "We can now compute the stationary points of this gradient by setting the gradient to 0:\n",
    "\n",
    "$$\n",
    "-2 (D_{\\cdot k}^\\top - X_{k \\cdot} Y^\\top) O_{Xk} Y = 0\n",
    "\\iff\n",
    "D_{ \\cdot k}^\\top Y (Y^\\top O_{Xk} Y)^{-1} = X_k\n",
    "$$\n",
    "\n",
    "The term $Y^\\top O_{Xk} Y$ needs to be invertible, but might not always be, depending on the exact ratings we have to work with. To handle potential non-invertibility, we add regularization terms to the objective function:\n",
    "\n",
    "$$\n",
    "\\min_{X,Y} \\|D - O \\circ (YX^\\top)\\|^2 + \\lambda\\|X\\|^2 + \\lambda\\|Y\\|^2\n",
    "$$\n",
    "\n",
    "This leads to well-defined stationary points:\n",
    "\n",
    "$$\n",
    "D_{\\cdot k}^\\top Y(Y^\\top O_{Xk}Y + \\lambda I)^{-1} = X_{k\\cdot}\n",
    "$$\n",
    "\n",
    "Doing the same for rows of $Y$ and solving the problem iteratively results in the \"block coordinate descent\" algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textbf{function } \\text{MatrixCompletion}(D, r; \\text{tmax} = 100, \\lambda = 0.1): \\\\\n",
    "\\hspace{1em} \\text{1. } (X, Y) \\leftarrow \\text{InitRandom}(n, d, r) \\\\\n",
    "\\hspace{1em} \\text{2. } O \\leftarrow \\text{IndicatorNonzero}(D) \\\\\n",
    "\\hspace{1em} \\text{3. } \\textbf{for } t \\in \\{1, \\ldots, \\text{tmax}\\}: \\\\\n",
    "\\hspace{3em} \\text{a. } \\textbf{for } k \\in \\{1, \\ldots, d\\}: \\\\\n",
    "\\hspace{5em} \\text{i. } O^X_k \\leftarrow \\text{diag}(O_{1k}, \\ldots, O_{nk}) \\\\\n",
    "\\hspace{5em} \\text{ii. } X_{k\\cdot} \\leftarrow D_{\\cdot k}^{\\top} Y (Y^{\\top} O^X_k Y + \\lambda I)^{-1} \\\\\n",
    "\\hspace{3em} \\text{b. } \\textbf{for } i \\in \\{1, \\ldots, n\\}: \\\\\n",
    "\\hspace{5em} \\text{i. } O^Y_i \\leftarrow \\text{diag}(O_{i1}, \\ldots, O_{id}) \\\\\n",
    "\\hspace{5em} \\text{ii. } Y_{i\\cdot} \\leftarrow D_{i\\cdot} X (X^{\\top} O^Y_i X + \\lambda I)^{-1} \\\\\n",
    "\\hspace{1em} \\text{4. } \\textbf{return } (X, Y)\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_eL3eJ2qPje"
   },
   "source": [
    "## 3a. Implementation and Evaluation\n",
    "\n",
    "Implement the `MatrixCompletion` function and apply it to the MovieLens data matrix $D$. Use the following default parameters:\n",
    "- Maximum iterations: $t_{max} = 100$\n",
    "- Rank: $r = 20$\n",
    "- Regularization parameter: $\\lambda = 0.1$\n",
    "\n",
    "After running the algorithm, report:\n",
    "\n",
    "1. The mean squared approximation error on the observed entries after 100 iterations:\n",
    "   $\\frac{1}{|O|}\\|D - O \\circ (YX^\\top)\\|^2 = $\n",
    "\n",
    "2. The estimated ratings for the first user in the dataset:\n",
    "   - Jumanji (1995): $JU = $\n",
    "   - Fight Club (1999): $FC = $\n",
    "   - Matrix, The (1999): $MT = $\n",
    "   - Monty Python and the Holy Grail (1975): $MP = $\n",
    "\n",
    "## 3b. Effect of Regularization\n",
    "\n",
    "Investigate how the regularization parameter $\\lambda$ affects the matrix completion results. Run the algorithm with different values of $\\lambda \\in \\{0.01, 0.1, 0.5\\}$ and observe the effects.\n",
    "\n",
    "Select all statements that are true:\n",
    "\n",
    "- [ ] The higher $\\lambda$ is, the higher the approximation error $\\|D - O \\circ (YX^\\top)\\|^2$ is for a single round of block updates.\n",
    "- [ ] The higher $\\lambda$ is, the lower the variance of the missing value imputations ($YX^\\top$ at positions where $O=0$) is.\n",
    "- [ ] The higher $\\lambda$ is, the lower the mean of the missing value imputations ($YX^\\top$ at positions where $O=0$) is.\n",
    "- [ ] The higher $\\lambda$ is, the fewer missing value imputations ($YX^\\top$ at positions where $O=0$) are outside of the original range of ratings in [0.5,5].\n",
    "\n",
    "Additionally, report the ratings for each new $\\lambda$ for the movie \"Monty Python and the Holy Grail (1975)\":\n",
    "\n",
    "- Rating $MP_{\\lambda=0.01} = $\n",
    "- Rating $MP_{\\lambda=0.5} = $\n",
    "\n",
    "**Optional (not graded):**\n",
    "- Plot the approximation error as a function of $\\lambda$\n",
    "- Compare the convergence rate for different $\\lambda$ values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2m1EEtaxqjuK"
   },
   "source": [
    "# 4. PCA for Network Intrusion Detection\n",
    "\n",
    "Imagine you’ve just joined the **cybersecurity team** of a large company.  \n",
    "One morning, your manager calls you in:  \n",
    "\n",
    "> “We’re flooded with thousands of network connection logs every minute.  \n",
    "> Some of them are normal, but a few might be **intrusion attempts**.  \n",
    "> The problem is… we don’t actually know which ones are malicious.  \n",
    "> Can you help us detect suspicious activity without prior labels?”  \n",
    "\n",
    "This is exactly where **Principal Component Analysis (PCA)** comes in.\n",
    "\n",
    "The idea:  \n",
    "1. Train PCA on a random sample of network data.  \n",
    "2. Reconstruct the connections from their compressed representation.  \n",
    "3. Use the **reconstruction error** as an anomaly score where large errors may signal an intrusion.  \n",
    "\n",
    "\n",
    "In this exercise, we’ll apply PCA to the **KDD Cup 1999 dataset** for intrusion detection.  \n",
    "\n",
    "PCA is a dimensionality reduction technique that identifies directions of maximum variance in high-dimensional data. Here, we'll apply it to a cybersecurity problem concerning network connection records in an **unsupervised setting** where we assume no prior knowledge of which connections are normal or intrusions.\n",
    "\n",
    "The dataset is loaded in the accompanying notebook (`D_balanced`). Each row (observation) corresponds to a network connection described by numerical and encoded categorical features, with a corresponding target label indicating that the row is \"normal\" (True) or an \"intrusion\" (False) in the vector `is_normal_balanced`. **Note: These labels are only provided for educational evaluation purposes - in practice, such labels would not be available.**\n",
    "\n",
    "We'll train PCA on a small random sample of the data and use reconstruction error to identify anomalous connections. The approach is based on the following low-rank matrix factorization:\n",
    "\n",
    "$$\n",
    "D - 1\\mu_C^\\top = C \\approx YX^\\top\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $X$ represents the principal components (directions of maximum variance)\n",
    "* $Y$ contains the low-dimensional representations of the data points\n",
    "* $\\mu_C$ is the mean connection vector\n",
    "* $D$ is the original data matrix\n",
    "* $C$ is the centered data matrix\n",
    "\n",
    "**Note.** When reconstructing the original matrix from the low-rank approximation $\\hat{D}$, the mean vector $\\mu_C$ must be added back to the product:\n",
    "$$\n",
    "\\hat{D} = YX^\\top\n",
    "$$\n",
    "\n",
    "Implement the PCA algorithm using the pseudocode provided in the slides. You may use the `sklearn` implementation for truncated SVD or perform a full SVD with `scipy` or `numpy` and truncate manually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLGQ-gEsqx-Q"
   },
   "source": [
    "\n",
    "## 4a. Low-Dimensional Representation\n",
    "\n",
    "Compute a PCA with rank $r=10$ using a **random sample of 100 intrusion data points** from the dataset (using the data loading code in the setup notebook). **In an unsupervised setting, we would simply take a random sample, but for this educational exercise we'll use a specific subset.** Determine the low-dimensional coordinates of the 8th data point (Python index 7) from your balanced analysis dataset.\n",
    "\n",
    "Report coordinates:\n",
    "\n",
    "* $y_0 = $\n",
    "* $y_1 = $\n",
    "* $y_2 = $\n",
    "\n",
    "## 4b. Outlier Detection via Reconstruction Error\n",
    "\n",
    "Use the trained PCA model to compute reconstruction errors for all data points. Points with high reconstruction error don't fit the learned patterns well and can be flagged as potential anomalies. Compute the reconstruction error:\n",
    "\n",
    "$$\n",
    "\\text{error}_i = \\| d_i - \\hat{d_i} \\|_2^2\n",
    "$$\n",
    "\n",
    "With $d_i$ indicating the value of the $i$-th point in $D$, and $\\hat{d_i}$ the reconstruction of the value of the $i$-th point.\n",
    "\n",
    "* Report the reconstruction error of the 8th connection\n",
    "* Report the mean reconstruction error of all connections labeled as \"intrusion\"\n",
    "* Report the mean reconstruction error of all connections labeled as \"normal\"\n",
    "\n",
    "**Optional (not graded):**\n",
    "* Plot box plots to gain an indication of the distribution of errors\n",
    "\n",
    "## 4c. Threshold Selection for Anomaly Detection\n",
    "\n",
    "To effectively use reconstruction error for anomaly detection, you need to determine an appropriate threshold that separates normal connections from outliers. A common approach is to analyze the distribution of reconstruction errors and identify natural breakpoints.\n",
    "\n",
    "Analyze the error distribution:\n",
    "* Sort all reconstruction errors in ascending order\n",
    "* Plot the reconstruction errors with respect to their percentile\n",
    "* Look for large consecutive differences, which indicate natural separation points in the data\n",
    "\n",
    "Normally, you do not have access to labels, so you would need to analyse the outliers (or hire an expert to do so) to know whether your outlier detection succeeded. For the sake of this exercise, suppose we have done so (luckily, you already have labels, since the expert is expensive and just left for a well-deserved holiday in Spain). Report how many true positive intrusion data points $tp$ are located in the different percentiles of the reconstruction error distribution. Also report the number of false positive points $fp$ (i.e., number of points flagged as outliers that are not intrusions).\n",
    "\n",
    "* For $90th$ percentile: $tp_{90}$= ,$fp_{90}= $\n",
    "* For $95th$ percentile: $tp_{95}$= ,$fp_{95}= $\n",
    "* For $99th$ percentile: $tp_{99}$= ,$fp_{99}= $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HQwBUrirYgz"
   },
   "source": [
    "# 5. k-means Initialization\n",
    "\n",
    "Imagine you are a **botanist** studying flowers in a large botanical garden.  \n",
    "You have collected measurements of three species of iris flowers (sepal length, sepal width, petal length, petal width), but you **don’t know the species labels** for some of them.  \n",
    "\n",
    "> You want to see if there are natural groups among these flowers based on their measurements.  \n",
    "> Your task is to group similar flowers together without looking at the species labels?”\n",
    "\n",
    "This is a perfect scenario for **k-means clustering**, a fundamental algorithm in **unsupervised learning**.  \n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "k-means clustering is a fundamental algorithm in unsupervised learning that groups similar data points together. The algorithm works by:\n",
    "1. Assigning each data point to the nearest cluster center (centroid).\n",
    "2. Updating the centroids by computing the mean of all points in each cluster.\n",
    "3. Repeating these steps until the clusters stabilize.\n",
    "\n",
    "However, the quality of the final clustering heavily depends on the initial positions of the centroids. Poor initialization can lead to suboptimal clustering results, slow convergence and getting stuck in local minima. This is why smart initialization techniques, like [greedy k-means++](https://arxiv.org/pdf/2207.07949v1), are crucial.\n",
    "\n",
    "K-means++ initialization is a smarter way to choose initial centroids. Instead of picking random points, it starts by selecting a random point as the first centroid. Then for each subsequent centroid, it chooses points that are far from the existing centroids. This helps spread out the initial centroids.\n",
    "\n",
    "Given a matrix $X \\in \\mathbb{R}^{d \\times r}$, we define the distance from a vector $v$ to the closest column in $X$ as:\n",
    "$dist(v,X) = \\min\\{\\|v - X_{\\cdot s}\\|_2 \\mid 1 \\leq s \\leq r\\}$\n",
    "\n",
    "Here's the pseudocode for the k-means++ initialization:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textbf{function } \\text{initGreedyKMeans}(D, r, l): \\\\\n",
    "\\hspace{1em} \\text{1. } \\text{Sample $i_1,\\ldots,i_l \\in \\{1,\\ldots,n\\}$ uniformly at random} \\\\\n",
    "\\hspace{1em} \\text{2. } i \\leftarrow \\text{arg min}_{i \\in \\{i_1,\\ldots,i_l\\}} \\sum\\limits_{j=1}^n \\|D_{j\\cdot} - D_{i\\cdot}\\|^2 \\\\\n",
    "\\hspace{1em} \\text{3. } X \\leftarrow D_{i\\cdot}^{\\top} \\\\\n",
    "\\hspace{1em} \\text{4. } s \\leftarrow 2 \\\\\n",
    "\\hspace{1em} \\text{5. } \\textbf{while } s \\leq r: \\\\\n",
    "\\hspace{3em} \\text{a. } \\text{Sample $i_1,\\ldots,i_l$ with probability $p(i) = \\frac{\\text{dist}(D_{i\\cdot}^{\\top},X)}{\\sum_{j=1}^n \\text{dist}(D_{j\\cdot}^{\\top},X)}$} \\\\\n",
    "\\hspace{3em} \\text{b. } i \\leftarrow \\text{arg min}_{i \\in \\{i_1,\\ldots,i_l\\}} \\sum\\limits_{j=1}^n \\text{dist}(D_{j\\cdot}^{\\top},[X \\; D_{i\\cdot}^{\\top}]) \\\\\n",
    "\\hspace{3em} \\text{c. } X \\leftarrow [X \\; D_{i\\cdot}^{\\top}] \\text{ // Add the new centroid to $X$} \\\\\n",
    "\\hspace{3em} \\text{d. } s \\leftarrow s + 1 \\\\\n",
    "\\hspace{1em} \\text{6. } \\textbf{return } X\n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83to8VmwrfO8"
   },
   "source": [
    "## 5a. Understanding k-means++ Initialization\n",
    "\n",
    "Let's examine the mathematical properties of the k-means++ initialization technique. Consider the following statements about the algorithm shown above:\n",
    "\n",
    "1. For a given centroid matrix $X \\in \\mathbb{R}^{d \\times r}$, we have the following equality:\n",
    "   $$\n",
    "   \\sum_{i=1}^n dist(D_{i\\cdot}^\\top, X) = \\min_Y \\|D - YX^\\top\\|^2 \\text{ s.t. } Y \\in \\mathbb{1}^{n \\times r}\n",
    "   $$\n",
    "\n",
    "2. The selection of the centroid in line 5.b is equivalent to choosing the centroid among the candidates with maximum probability:\n",
    "   $$\n",
    "   i \\leftarrow \\text{arg max}_{i \\in \\{i_1,\\ldots,i_l\\}} p(i)\n",
    "   $$\n",
    "\n",
    "3. The probability that data point $D_{i\\cdot}$ is sampled as a centroid candidate in line 5.a is higher, the further away the data point is from all centroids chosen so far.\n",
    "\n",
    "4. The first centroid in line 2 is chosen as the one that would minimize the 1-means problem the most among the given candidates:\n",
    "   $$\n",
    "   i \\leftarrow \\text{arg min}_{i \\in \\{i_1,\\ldots,i_l\\}} \\|D - \\mathbb{1}D_{i\\cdot}\\|^2\n",
    "   $$\n",
    "   where $\\mathbb{1}$ is the $n$-dimensional constant one vector.\n",
    "\n",
    "Which of these statements is incorrect?\n",
    "\n",
    "## 5b. Clustering Evaluation\n",
    "\n",
    "In this part, we'll test the k-means++ initialization on a dataset provided in the setup notebook.\n",
    "\n",
    "2. Apply k-means clustering using your k-means++ initialization with:\n",
    "   - Number of clusters: $r = 3$ (matching the ground truth)\n",
    "   - Initialization parameter: $l = 10$ (number of candidates to consider for each centroid)\n",
    "\n",
    "3. Evaluate the results using:\n",
    "   - Mean approximation error: $\\frac{1}{nd}\\|D - YX^\\top\\|^2 = $\n",
    "   - Normalized Mutual Information (NMI) score: $nmi = $\n",
    "\n",
    "**Optional (not graded):**\n",
    "- Visualize the clustering results to see how well the algorithm separated the blobs of data\n",
    "- Try different values of $l$ to understand its impact on the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLlFqnL_Qc9y"
   },
   "source": [
    "# 6. Image Classification With Neural Networks\n",
    "\n",
    "In this exercise, we'll explore neural network architectures using PyTorch, focusing on the MNIST dataset. The implementation includes various modern neural network components like convolutional layers, residual blocks, and batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2U4v9H9YCIya"
   },
   "source": [
    "### 6a. Understanding Network Architecture  \n",
    "\n",
    "Imagine you’ve joined a **startup developing smart document scanners**.  \n",
    "Your product needs to automatically recognize handwritten digits on forms — for example, zip codes, invoice numbers, or survey responses.  \n",
    "\n",
    "> “We want our scanner to be fast and accurate, even with messy handwriting.  \n",
    "> Can you build a neural network that can classify digits from images correctly?”  \n",
    "\n",
    "For this exercise, we’ll use the **MNIST dataset**, a standard benchmark of 28×28 grayscale images of handwritten digits (0–9).  \n",
    "\n",
    "You are provided with a **PyTorch implementation** of a neural network that includes:\n",
    "\n",
    "- **Convolutional layers** for extracting spatial features  \n",
    "- **Residual blocks** for efficient training  \n",
    "- **Batch normalization** for stable learning  \n",
    "- **Dropout** for regularization  \n",
    "\n",
    "\n",
    "#### Network Setup:\n",
    "\n",
    "```python\n",
    "class ResidualBlock(nn.Module):\n",
    "    # Residual block implementation\n",
    "    # Contains convolutional layers, batch normalization, and skip connections\n",
    "\n",
    "class EmbeddingNetwork(nn.Module):\n",
    "    # Main network architecture\n",
    "    # Processes input through convolutional layers, residual blocks, and fully connected layers\n",
    "\n",
    "    def __init__(self, embedding_dim: int, dropout_rate_1: float = 0.3, dropout_rate_2: float = 0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initial feature extraction\n",
    "        self.initial_conv = nn.Conv2d(1, 32, kernel_size=5, padding=\"same\")\n",
    "        self.initial_norm = nn.BatchNorm2d(32)\n",
    "\n",
    "        # First residual block set (32 channels, groups=2)\n",
    "        self.res_block1 = ResidualBlock(32, 32, kernel_size=3, groups=2)\n",
    "        self.res_block2 = ResidualBlock(32, 32, kernel_size=3, groups=2)\n",
    "\n",
    "        # Spatial downsampling\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.norm_after_pool = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Second residual block set (64 channels, groups=4)\n",
    "        self.res_block3 = ResidualBlock(32, 64, kernel_size=3, groups=4)\n",
    "        self.res_block4 = ResidualBlock(64, 64, kernel_size=3, groups=4)\n",
    "\n",
    "        # Final processing\n",
    "        self.final_norm = nn.BatchNorm1d(64)\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, embedding_dim)\n",
    "\n",
    "        # Regularization\n",
    "        self.dropout1 = nn.Dropout(dropout_rate_1)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate_2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass mapping images to embedding space.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, 1, 28, 28)\n",
    "\n",
    "        Returns:\n",
    "            Embedding tensor of shape (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        out = F.relu(self.initial_norm(self.initial_conv(x)))\n",
    "        # out.shape = ? (tensor shape 1)\n",
    "\n",
    "        out = self.res_block2(self.res_block1(out))\n",
    "        # out.shape = ? (tensor shape 2)\n",
    "\n",
    "        out = self.norm_after_pool(self.pool(out))\n",
    "        # out.shape = ? (tensor shape 3)\n",
    "\n",
    "        out = self.res_block4(self.res_block3(out))\n",
    "        # out.shape = ? (tensor shape 4)\n",
    "\n",
    "        out = torch.mean(out, dim=(-1, -2))\n",
    "        out = self.final_norm(out)\n",
    "        # out.shape = ? (tensor shape 5)\n",
    "\n",
    "        out = self.dropout1(out)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "\n",
    "    # Complete model combining embedding network with classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nkPz7PpK0j0"
   },
   "source": [
    "Your first task is to **analyze the network**. Specifically:\n",
    "\n",
    "- Consider a batch of **128 MNIST images** (shape `(128, 1, 28, 28)`).  \n",
    "- Track the **tensor shapes** at different stages of the network as they pass through the convolutional layers, residual blocks, and batch normalization layers.  \n",
    "- Understanding these shapes is crucial for debugging, designing new layers, and building intuition for how deep networks process images.\n",
    "---\n",
    "\n",
    "1. **After initial convolution and normalization**  \n",
    "   Output shape:  \n",
    "   `(b, c₁, h₁, w₁) = (128, 32, 28, 28)`\n",
    "\n",
    "2. **After first set of residual blocks**  \n",
    "   Output shape:  \n",
    "   `(b, c₂, h₂, w₂) = (128, 32, 28, 28)`\n",
    "\n",
    "3. **After pooling layer**  \n",
    "   Output shape:  \n",
    "   `(b, c₃, h₃, w₃) = (128, 32, 14, 14)`\n",
    "\n",
    "4. **After second set of residual blocks**  \n",
    "   Output shape:  \n",
    "   `(b, c₄, h₄, w₄) = (128, 64, 14, 14)`\n",
    "\n",
    "5. **After spatial averaging and batch normalization**  \n",
    "   Output shape:  \n",
    "   `(b, c₅) = (128, 64)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1t93UESoK7kA"
   },
   "source": [
    "### 6b. Training and Analysis (Open Question)\n",
    "\n",
    "Continuing from the handwriting scanner scenario, imagine you’re now responsible for **training the neural network** and understanding how it makes decisions.  \n",
    "\n",
    "---\n",
    "\n",
    "### Part 1: Hyperparameter Tuning  \n",
    "\n",
    "Your goal is to make the scanner **accurate enough for real-world use** — specifically, achieve **at least 95% accuracy** on the MNIST test set.  \n",
    "\n",
    "- Run the training with different hyperparameter settings.  \n",
    "- Report your best performing configuration:  \n",
    "\n",
    "---\n",
    "\n",
    "### Part 2: Understanding the Learned Representations  \n",
    "\n",
    "The network learns to represent MNIST digits in a **2D embedding space**.  \n",
    "Because we choose the representation to be in two dimensions, we can plot the embedding space and analyze what the network has learned.  \n",
    "\n",
    "Use the provided visualization function to plot the representations. In your answer, describe:  \n",
    "\n",
    "1. Where are the decision boundaries between different digits?  \n",
    "2. Which areas of the plot show high confidence in the predictions?  \n",
    "3. How are the test images distributed in this 2D space?  \n",
    "\n",
    "---\n",
    "\n",
    "The final layer of the network is a **linear classifier**.  \n",
    "\n",
    "- Report the learned weight matrix **W**.  \n",
    "- Explain how this matrix creates the decision boundaries you observed in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Part 1:***\n",
    "\n",
    "Best hyperparameter configuration is: \n",
    "    embedding_dim = 2,      \n",
    "    num_classes   = 10,\n",
    "    epochs        = 15,     \n",
    "    learning_rate = 0.10,   \n",
    "    momentum      = 0.9,\n",
    "    weight_decay  = 5e-4,\n",
    "    batch_size    = 128,\n",
    "    dropout_rate_1 = 0.30,  \n",
    "    dropout_rate_2 = 0.30\n",
    "\n",
    "MNIST Test Accuracy: 97.80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Part 2:***\n",
    "1. Where are the decision boundaries between different digits?  \n",
    "\n",
    "In the 2D embedding, decision boundaries are straight lines radiating from the center, as expected from a linear head where ties satisfy $(\\mathbf{w}_i - \\mathbf{w}_j)^\\top \\mathbf{z} + (b_i - b_j) = 0$ each wedge between lines corresponds to one digit class.\n",
    "\n",
    "2. Which areas of the plot show high confidence in the predictions?  \n",
    "\n",
    "Confidence is highest deep inside a wedge—far from any boundary—because one class logit dominates; it is lowest on the lines themselves, where competing classes have similar scores.\n",
    "\n",
    "3. How are the test images distributed in this 2D space?  \n",
    "\n",
    "Test images form class-specific wedges: 1s to the right, 4s upper-right, 7s lower-right, 0s upper-left, 6s mostly above center, while 3/5/8 gather toward the bottom/left with some overlap; samples near line intersections appear ambiguous, whereas those well inside a wedge are high-confidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization: \n",
    "\n",
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Report the learned weight matrix **W**.  \n",
    "\n",
    "W =\n",
    "[[-1.6789,  1.4596],\n",
    " [ 2.1098,  0.3485],\n",
    " [ 0.4491, -1.6554],\n",
    " [-0.4303, -2.2761],\n",
    " [ 0.9728,  1.8243],\n",
    " [-1.5348, -1.4022],\n",
    " [-0.5104,  2.1061],\n",
    " [ 2.0191, -0.9794],\n",
    " [-2.2033, -0.0437],\n",
    " [ 0.8081,  0.6187]]\n",
    "\n",
    "b =\n",
    "[-0.9944, -1.2587,  2.4261, -2.4263, -0.9879,\n",
    "  0.1610, -0.4652, -0.7882,  0.3535,  3.9801]\n",
    "\n",
    "\n",
    "- Explain how this matrix creates the decision boundaries you observed in the plot.\n",
    "\n",
    "The final linear layer maps each 2-D embedding $z$ to class scores $\\text{logit}_k = \\mathbf{w}_k^\\top \\mathbf{z} + b_k$ where the k-th row of the weight matrix W is $w_k$. A decision boundary between classes $i$ and $j$ occurs where their scores tie, i.e.,$(\\mathbf{w}_i - \\mathbf{w}_j)^\\top \\mathbf{z} + (b_i - b_j) = 0$. In 2D this is a straight line whose orientation is perpendicular to $(\\mathbf{w}_i - \\mathbf{w}_j)$ and whose position is shifted by the bias difference $(\\mathbf{b}_i - \\mathbf{b}_j)$. Thus, the rows of $W$ act like “direction vectors” pointing toward the centers of each class’s wedge in the plot, while the biases slide the separating lines inward or outward, yielding the star-shaped tessellation you observed. Softmax then turns distances from these lines into probabilities, so points far along a class’s $w_k$ direction are high-confidence, and points near any boundary line are low-confidence.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
