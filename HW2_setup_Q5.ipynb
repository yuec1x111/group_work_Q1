{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "a_tqWHuVczNn"
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Packages related to NN Question\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Optional, Callable\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgByXe1Jc_7h"
   },
   "source": [
    "# 1. Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FjLNFBVTSk_z"
   },
   "outputs": [],
   "source": [
    "# This is the helper function to load the dataset. PLEASE DO NOT CHANGE THIS.\n",
    "\n",
    "# --- Dataset ---\n",
    "\n",
    "rows = [\n",
    "{\"ID\":1, \"Age\":\"<=30\", \"Income\":\"Average\", \"Student\":\"No\", \"Credit\":\"Fair\", \"Buy\":\"No\"},\n",
    "{\"ID\":2, \"Age\":\"<=30\", \"Income\":\"High\", \"Student\":\"Yes\", \"Credit\":\"Good\", \"Buy\":\"No\"},\n",
    "{\"ID\":3, \"Age\":\"30-40\", \"Income\":\"High\", \"Student\":\"No\", \"Credit\":\"Fair\", \"Buy\":\"Yes\"},\n",
    "{\"ID\":4, \"Age\":\">40\", \"Income\":\"Low\", \"Student\":\"No\", \"Credit\":\"Fair\", \"Buy\":\"Yes\"},\n",
    "{\"ID\":5, \"Age\":\">40\", \"Income\":\"Low\", \"Student\":\"Yes\", \"Credit\":\"Good\", \"Buy\":\"Yes\"},\n",
    "{\"ID\":6, \"Age\":\">40\", \"Income\":\"Average\", \"Student\":\"Yes\", \"Credit\":\"Good\", \"Buy\":\"No\"},\n",
    "{\"ID\":7, \"Age\":\"30-40\", \"Income\":\"Low\", \"Student\":\"Yes\", \"Credit\":\"Good\", \"Buy\":\"Yes\"},\n",
    "{\"ID\":8, \"Age\":\"<=30\", \"Income\":\"Average\", \"Student\":\"No\", \"Credit\":\"Fair\", \"Buy\":\"No\"},\n",
    "{\"ID\":9, \"Age\":\"<=30\", \"Income\":\"Low\", \"Student\":\"No\", \"Credit\":\"Fair\", \"Buy\":\"Yes\"},\n",
    "{\"ID\":10,\"Age\":\">40\", \"Income\":\"Average\", \"Student\":\"Yes\", \"Credit\":\"Fair\", \"Buy\":\"Yes\"},\n",
    "{\"ID\":11,\"Age\":\"<=30\", \"Income\":\"Average\", \"Student\":\"Yes\", \"Credit\":\"Good\", \"Buy\":\"Yes\"},\n",
    "{\"ID\":12,\"Age\":\"30-40\", \"Income\":\"High\", \"Student\":\"No\", \"Credit\":\"Good\", \"Buy\":\"Yes\"},\n",
    "{\"ID\":13,\"Age\":\"30-40\", \"Income\":\"Average\", \"Student\":\"Yes\", \"Credit\":\"Fair\", \"Buy\":\"Yes\"},\n",
    "{\"ID\":14,\"Age\":\">40\", \"Income\":\"Average\", \"Student\":\"No\", \"Credit\":\"Good\", \"Buy\":\"No\"},\n",
    "]\n",
    "\n",
    "# We need to convert it to pandas dataframe format.\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ybqBssYXS5iE"
   },
   "outputs": [],
   "source": [
    "# Please IMPLEMENT your code for the below function\n",
    "\n",
    "def conditional_prob(df_class, feature, value, alpha):\n",
    "    \"\"\"\n",
    "    Return P(feature = value | class)\n",
    "    df_class: subset of dataframe for the class label\n",
    "    feature: column name\n",
    "    value: feature value to query\n",
    "    alpha: laplace smoothing parameter\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zyaYzhK_VBmM"
   },
   "outputs": [],
   "source": [
    "# Your remaining code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mta_D8n8dDEO"
   },
   "source": [
    "# 2. Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "s58YCrMGX0yp"
   },
   "outputs": [],
   "source": [
    "# This is the helper function to load the dataset. PLEASE DO NOT CHANGE THIS.\n",
    "\n",
    "# --- Dataset ---\n",
    "rows = [\n",
    "    {\"ID\":1, \"Age\":\"<=30\",   \"Income\":\"Average\",  \"Student\":\"No\",  \"Credit\":\"Fair\",  \"Buy\":\"No\"},\n",
    "    {\"ID\":2, \"Age\":\"<=30\",   \"Income\":\"High\",     \"Student\":\"Yes\", \"Credit\":\"Good\",  \"Buy\":\"No\"},\n",
    "    {\"ID\":3, \"Age\":\"30-40\",  \"Income\":\"High\",     \"Student\":\"No\",  \"Credit\":\"Fair\",  \"Buy\":\"Yes\"},\n",
    "    {\"ID\":4, \"Age\":\">40\",    \"Income\":\"Low\",      \"Student\":\"No\",  \"Credit\":\"Fair\",  \"Buy\":\"Yes\"},\n",
    "    {\"ID\":5, \"Age\":\">40\",    \"Income\":\"Low\",      \"Student\":\"Yes\", \"Credit\":\"Good\",  \"Buy\":\"Yes\"},\n",
    "    {\"ID\":6, \"Age\":\">40\",    \"Income\":\"Average\",  \"Student\":\"Yes\", \"Credit\":\"Good\",  \"Buy\":\"No\"},\n",
    "    {\"ID\":7, \"Age\":\"30-40\",  \"Income\":\"Low\",      \"Student\":\"Yes\", \"Credit\":\"Good\",  \"Buy\":\"Yes\"},\n",
    "    {\"ID\":8, \"Age\":\"<=30\",   \"Income\":\"Average\",  \"Student\":\"No\",  \"Credit\":\"Fair\",  \"Buy\":\"No\"},\n",
    "    {\"ID\":9, \"Age\":\"<=30\",   \"Income\":\"Low\",      \"Student\":\"No\",  \"Credit\":\"Fair\",  \"Buy\":\"Yes\"},\n",
    "    {\"ID\":10,\"Age\":\">40\",    \"Income\":\"Average\",  \"Student\":\"Yes\", \"Credit\":\"Fair\",  \"Buy\":\"Yes\"},\n",
    "    {\"ID\":11,\"Age\":\"<=30\",   \"Income\":\"Average\",  \"Student\":\"Yes\", \"Credit\":\"Good\",  \"Buy\":\"Yes\"},\n",
    "    {\"ID\":12,\"Age\":\"30-40\",  \"Income\":\"High\",     \"Student\":\"No\",  \"Credit\":\"Good\",  \"Buy\":\"Yes\"},\n",
    "    {\"ID\":13,\"Age\":\"30-40\",  \"Income\":\"Average\",  \"Student\":\"Yes\", \"Credit\":\"Fair\",  \"Buy\":\"Yes\"},\n",
    "    {\"ID\":14,\"Age\":\">40\",    \"Income\":\"Average\",  \"Student\":\"No\",  \"Credit\":\"Good\",  \"Buy\":\"No\"},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KGK3XW7cX7bx"
   },
   "outputs": [],
   "source": [
    "# Please IMPLEMENT your code for the below function\n",
    "\n",
    "def entropy(target_col):\n",
    "    \"\"\"Compute entropy of a pandas Series\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RummTqLTYS1X"
   },
   "outputs": [],
   "source": [
    "# Please IMPLEMENT your code for the below function\n",
    "\n",
    "def info_gain(df, split_attr, target='Buy'):\n",
    "    \"\"\"Compute information gain of splitting df by split_attr\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GicZ2pq_YWq6"
   },
   "outputs": [],
   "source": [
    "# Your remaining code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLbo-6YaqZY1"
   },
   "source": [
    "# 3. Netflix Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IsAWfBMUqchb"
   },
   "outputs": [],
   "source": [
    "# This is the helper function to load the dataset. PLEASE DO NOT CHANGE THIS.\n",
    "\n",
    "def load_ratings_data_pandas(data_dir=\"ml-latest-small/\"):\n",
    "    \"\"\"Load data using pandas dataframes.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    assert data_dir.exists(), f\"{data_dir} does not exist\"\n",
    "\n",
    "    return pd.read_csv(data_dir / 'ratings.csv',sep=',')\n",
    "\n",
    "\n",
    "def load_movies_data_pandas(data_dir=\"ml-latest-small/\"):\n",
    "    \"\"\"Load data using pandas dataframes.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    assert data_dir.exists(), f\"{data_dir} does not exist\"\n",
    "    return pd.read_csv(data_dir / 'movies.csv')\n",
    "\n",
    "def filter_data(ratings_data: pd.DataFrame, movies_data: pd.DataFrame):\n",
    "    \"\"\"Filter data. Too little ratings prevent effective use of matrix completion.\"\"\"\n",
    "    ratings_data = ratings_data.pivot(\n",
    "        index='userId',\n",
    "        columns='movieId',\n",
    "        values='rating'\n",
    "    ).fillna(0)\n",
    "\n",
    "    keep_movie = (ratings_data != 0).sum(axis=0) > 100\n",
    "    ratings_data = ratings_data.loc[:, keep_movie]\n",
    "\n",
    "    # Filter movies_data by movieId (columns of ratings_data after filtering)\n",
    "    movies_data = movies_data[movies_data['movieId'].isin(ratings_data.columns)]\n",
    "\n",
    "    keep_user = (ratings_data != 0).sum(axis=1) >= 5\n",
    "    ratings_data = ratings_data.loc[keep_user, :]\n",
    "\n",
    "    return ratings_data, movies_data\n",
    "\n",
    "def print_data_summary(ratings: pd.DataFrame):\n",
    "    n_users = ratings.shape[0]\n",
    "    n_movies = ratings.shape[1]\n",
    "    n_ratings = (ratings != 0).sum().sum()\n",
    "    density = n_ratings / (n_users * n_movies)\n",
    "\n",
    "    print(f\"Dataset Summary\")\n",
    "    print(f\"----------------\")\n",
    "    print(f\"Users: {n_users}\")\n",
    "    print(f\"Movies: {n_movies}\")\n",
    "    print(f\"Total Ratings: {n_ratings}\")\n",
    "    print(f\"Data Density: {density:.4f} (fraction of observed ratings)\")\n",
    "\n",
    "def load_data_pandas(data_dir=\"ml-latest-small/\", print_summary=False):\n",
    "    \"\"\"Load data in pandas format.\"\"\"\n",
    "    ratings, movies = filter_data(\n",
    "        load_ratings_data_pandas(data_dir=data_dir),\n",
    "        load_movies_data_pandas(data_dir=data_dir)\n",
    "    )\n",
    "    if print_summary:\n",
    "        print_data_summary(ratings)\n",
    "    return ratings, movies\n",
    "\n",
    "\n",
    "def load_data(data_dir=\"ml-latest-small/\", print_summary=False):\n",
    "    \"\"\"Load data in numpy format.\"\"\"\n",
    "    ratings, movies = load_data_pandas(data_dir=data_dir, print_summary=print_summary)\n",
    "    return ratings.to_numpy(), movies.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wpV33c4-JHUW"
   },
   "outputs": [],
   "source": [
    "# Please IMPLEMENT your code for the below function and DONT forget to provide correct n_features (r),  t_max and lambda.\n",
    "# You can use the same function for Question 3b to run the matrix completion algorithm with different lambda values.\n",
    "\n",
    "def matrix_completion(D, n_features, n_movies, n_users, t_max, lambd):\n",
    "    np.random.seed(0)\n",
    "    X = np.random.normal(size=(n_movies, n_features))\n",
    "    Y = np.random.normal(size=(n_users, n_features))\n",
    "\n",
    "    # Implementation the optimization procedure here\n",
    "    for t in range(t_max):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "-tfXT3qxsZEQ",
    "outputId": "44ddccf3-e843-4d60-e3e9-a9cb05b64000"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "datasets/ml-latest-small does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3060365047.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasets/ml-latest-small\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-620278350.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(data_dir, print_summary)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ml-latest-small/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;34m\"\"\"Load data in numpy format.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprint_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-620278350.py\u001b[0m in \u001b[0;36mload_data_pandas\u001b[0;34m(data_dir, print_summary)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m\"\"\"Load data in pandas format.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     ratings, movies = filter_data(\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mload_ratings_data_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mload_movies_data_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     )\n",
      "\u001b[0;32m/tmp/ipython-input-620278350.py\u001b[0m in \u001b[0;36mload_ratings_data_pandas\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\"Load data using pandas dataframes.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{data_dir} does not exist\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'ratings.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: datasets/ml-latest-small does not exist"
     ]
    }
   ],
   "source": [
    "ratings, movies = load_data(\"datasets/ml-latest-small\", print_summary=True)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cdEmehc5J13Q"
   },
   "outputs": [],
   "source": [
    "# Please IMPLEMENT your code for the below function to retrieve estimated ratings for the movies from the first user.\n",
    "# You can use the same function for Question 3b to run the matrix completion algorithm\n",
    "# with different lambda values and report the rating for the movie \"Monty Python and the Holy Grail (1975)\".\n",
    "\n",
    "def get_predicted_ratings(titles, X, Y, ratings_df, movies_df, user=0):\n",
    "    \"\"\"\n",
    "    Retrieve predicted ratings for a list of movie titles for a specified user.\n",
    "\n",
    "    Args:\n",
    "        titles (list of str): Movie titles to predict ratings for.\n",
    "        X (np.ndarray): Movie feature matrix.\n",
    "        Y (np.ndarray): User feature matrix.\n",
    "        ratings_df (pd.DataFrame): Ratings matrix (users × movies).\n",
    "        movies_df (pd.DataFrame): DataFrame with movieId and title.\n",
    "        user (int, optional): User index. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary mapping movie titles to predicted ratings (float),\n",
    "              or None if the movie is not available in the dataset.\n",
    "    \"\"\"\n",
    "    # Your implementation starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOVwGWeidFmg"
   },
   "source": [
    "# 4. PCA for Network Intrusion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SAthOLI5dRbp"
   },
   "outputs": [],
   "source": [
    "# This is the helper function for loading the kdd_balanced dataset. PLEASE DO NOT CHANGE the below code.\n",
    "# This code will load the balanced kdd dataset. In the original version the number of intrusion samples are much more than normal samples.\n",
    "# In this balanced version we have 100 intrusion samples, and 97,278 normal samples.\n",
    "\n",
    "def load_kdd_data(data_dir=\"./datasets/kdd_balanced\"):\n",
    "    \"\"\"\n",
    "    Load the balanced dataset from Parquet format (ultra-fast loading).\n",
    "\n",
    "    Returns:\n",
    "        D_balanced: Feature matrix (numpy array)\n",
    "        is_normal_balanced: Boolean labels (numpy array)\n",
    "        original_indices: Original indices from full dataset (numpy array)\n",
    "        df_balanced: Original dataframe subset (if available)\n",
    "        metadata: Dataset metadata\n",
    "    \"\"\"\n",
    "\n",
    "    data_path = Path(data_dir)\n",
    "\n",
    "    # Check if required files exist\n",
    "    required_files = [\"balanced_dataset.parquet\", \"metadata.json\"]\n",
    "    missing_files = [f for f in required_files if not (data_path / f).exists()]\n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(f\"Required files not found in {data_path}: {missing_files}\")\n",
    "\n",
    "    # Load main dataset (this is very fast with Parquet)\n",
    "    df_main = pd.read_parquet(data_path / \"balanced_dataset.parquet\")\n",
    "\n",
    "    # Extract components\n",
    "    feature_cols = [col for col in df_main.columns if col.startswith('feature_')]\n",
    "    D_balanced = df_main[feature_cols].values\n",
    "    is_normal_balanced = df_main['is_normal'].values\n",
    "    original_indices = df_main['original_index'].values\n",
    "\n",
    "    # Load original dataframe if available\n",
    "    df_balanced = None\n",
    "    if (data_path / \"original_data_balanced.parquet\").exists():\n",
    "        df_balanced = pd.read_parquet(data_path / \"original_data_balanced.parquet\")\n",
    "\n",
    "    # Load metadata\n",
    "    with open(data_path / \"metadata.json\", 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    print(f\"Balanced dataset loaded from {data_path} (Parquet format)\")\n",
    "    print(f\"  Features: {D_balanced.shape}\")\n",
    "    print(f\"  Normal: {metadata['n_normal']:,}, Intrusion: {metadata['n_intrusion']:,}\")\n",
    "\n",
    "    return D_balanced, is_normal_balanced, original_indices, df_balanced, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "erGtM2WShPwN"
   },
   "outputs": [],
   "source": [
    "# Implement your code here.\n",
    "\n",
    "def compute_pca_from_intrusion_data(D_intrusion, r):\n",
    "    \"\"\"\n",
    "    Compute PCA using SVD on intrusion data with rank r.\n",
    "\n",
    "    Key insight: We train PCA on INTRUSION data, so normal data\n",
    "    will have higher reconstruction error in this learned space.\n",
    "\n",
    "    Returns:\n",
    "        X: Principal components (learned from intrusion data)\n",
    "        mu_intrusion: Mean vector of intrusion data\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "OoSB2PhFhP4P"
   },
   "outputs": [],
   "source": [
    "# Implement your code here.\n",
    "\n",
    "def project_to_low_dimensional_space(D, X, mu_intrusion):\n",
    "    \"\"\"\n",
    "    Project any data points into the low-dimensional space defined by\n",
    "    intrusion-trained PCA components.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "XPFgSUuxhUZE"
   },
   "outputs": [],
   "source": [
    "# Implement your code here.\n",
    "\n",
    "def reconstruct_from_low_dimensional(Y, X, mu_intrusion):\n",
    "    \"\"\"Reconstruct data points from low-dimensional coordinates.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "s44gv--whUbm"
   },
   "outputs": [],
   "source": [
    "# Implement your code here.\n",
    "\n",
    "def compute_reconstruction_error(original, reconstructed):\n",
    "    \"\"\"Compute L2 reconstruction error.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "H5TwhLjZhcaF",
    "outputId": "c7f7bb9f-d13a-4016-be4e-0c7d0415f8fb"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Required files not found in datasets/kdd_balanced: ['balanced_dataset.parquet', 'metadata.json']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2697293076.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mD_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_normal_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_kdd_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-319992881.py\u001b[0m in \u001b[0;36mload_kdd_data\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmissing_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrequired_files\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmissing_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Required files not found in {data_path}: {missing_files}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Load main dataset (this is very fast with Parquet)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Required files not found in datasets/kdd_balanced: ['balanced_dataset.parquet', 'metadata.json']"
     ]
    }
   ],
   "source": [
    "D_balanced, is_normal_balanced, original_indices, _, metadata = load_kdd_data()\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lRBHYw0pGAc"
   },
   "source": [
    "# 5. K-means Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5a. (1) and (2) incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZakdVGBwpTG8"
   },
   "outputs": [],
   "source": [
    "# This is the helper function for loading the dataset. Please DO NOT CHANGE the below code.\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def load_iris_data(path=Path(\"HW2_datasets/datasets/iris/iris.csv\")):\n",
    "    df = pd.read_csv(path)\n",
    "    data = df.iloc[:, :-1].values\n",
    "    labels = df['target'].values\n",
    "    return data, labels\n",
    "\n",
    "def print_iris_info(data, labels):\n",
    "    n_samples, n_features = data.shape\n",
    "    n_classes = len(np.unique(labels))\n",
    "    feature_names = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "\n",
    "    print(f\"Number of samples: {n_samples}\")\n",
    "    print(f\"Number of features: {n_features}\")\n",
    "    print(f\"Number of classes: {n_classes}\")\n",
    "    print(f\"Feature names: {feature_names}\")\n",
    "    print(f\"Class distribution:\")\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    for cls, count in zip(unique, counts):\n",
    "        print(f\"  Class {cls}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zAksVdXQMemX"
   },
   "outputs": [],
   "source": [
    "# This is the K-means implementation from the lecture slides. PLEASE DO NOT CHANGE the code below.\n",
    "\n",
    "def RSS(D,X,Y):\n",
    "    return np.sum((D- Y@X.T)**2)\n",
    "\n",
    "def getY(labels):\n",
    "    Y = np.eye(max(labels)+1)[labels]\n",
    "    return Y\n",
    "\n",
    "def update_centroid(D,Y):\n",
    "    cluster_sizes = np.diag(Y.T@Y).copy()\n",
    "    cluster_sizes[cluster_sizes==0]=1\n",
    "    return D.T@Y/cluster_sizes\n",
    "\n",
    "def update_assignment(D,X):\n",
    "    dist = np.sum((np.expand_dims(D,2) - X)**2,1)\n",
    "    labels = np.argmin(dist,1)\n",
    "    return getY(labels)\n",
    "\n",
    "def kmeans(D,r, X_init, epsilon=0.00001, t_max=10000):\n",
    "    X = X_init.copy()\n",
    "    Y = update_assignment(D,X)\n",
    "    rss_old = RSS(D,X,Y) +2*epsilon\n",
    "    t=0\n",
    "\n",
    "    #Looping as long as difference of objective function values is larger than epsilon\n",
    "    while rss_old - RSS(D,X,Y) > epsilon and t < t_max-1:\n",
    "        rss_old = RSS(D,X,Y)\n",
    "        X = update_centroid(D,Y)\n",
    "        Y = update_assignment(D,X)\n",
    "        t+=1\n",
    "    print(t,\"iterations\")\n",
    "    return X,Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "AVUPdx_TL-JT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    candidate_indexes = rng.integers(low=0, high=n, size=r)\\n    X = np.array(D[indexes,:]).T\\n    return X'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please implement your code here.\n",
    "\n",
    "def init_centroids_greedy_pp(D,r,l=10):\n",
    "    '''\n",
    "        :param r: (int) number of centroids (clusters)\n",
    "        :param D: (np-array) the data matrix\n",
    "        :param l: (int) number of centroid candidates in each step\n",
    "        :return: (np-array) 'X' the selected centroids from the dataset\n",
    "    '''\n",
    "    rng =  np.random.default_rng(seed=RANDOM_SEED) # use this random generator to sample the candidates (sampling according to given probabilities can be done via rng.choice(..))\n",
    "    n,d = D.shape\n",
    "\n",
    "    #-----------\n",
    "    candidate_indices = rng.choice(n, size=l, replace=False)\n",
    "    costs = [np.sum((D - D[idx, :])**2) for idx in candidate_indices]\n",
    "    first_centroid_idx = candidate_indices[np.argmin(costs)]\n",
    "    \n",
    "    X = D[first_centroid_idx, :].reshape(d, 1)\n",
    "    \n",
    "    s = 2\n",
    "    while s <= r:\n",
    "        dist_sq_all = np.sum((D[:, np.newaxis, :] - X.T)**2, axis=2)\n",
    "        min_dist_sq = np.min(dist_sq_all, axis=1)\n",
    "        \n",
    "        probabilities = min_dist_sq / np.sum(min_dist_sq)\n",
    "            \n",
    "        candidate_indices = rng.choice(n, size=l, p=probabilities)\n",
    "\n",
    "        candidate_costs = []\n",
    "        for cand_idx in candidate_indices:\n",
    "            candidate_centroid = D[cand_idx, :].reshape(d, 1)\n",
    "            X_temp = np.hstack((X, candidate_centroid))\n",
    "            \n",
    "            dist_sq_all_temp = np.sum((D[:, np.newaxis, :] - X_temp.T)**2, axis=2)\n",
    "            total_cost = np.sum(np.min(dist_sq_all_temp, axis=1))\n",
    "            candidate_costs.append(total_cost)\n",
    "            \n",
    "        best_candidate_idx = candidate_indices[np.argmin(candidate_costs)]\n",
    "        \n",
    "        new_centroid = D[best_candidate_idx, :].reshape(d, 1)\n",
    "        X = np.hstack((X, new_centroid))\n",
    "        \n",
    "        s += 1\n",
    "        \n",
    "    return X\n",
    "    \n",
    "\n",
    "\"\"\"    candidate_indexes = rng.integers(low=0, high=n, size=r)\n",
    "    X = np.array(D[indexes,:]).T\n",
    "    return X\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "id": "-KcQuHnNMusy",
    "outputId": "1ca0c1a5-db03-4507-f091-6919ee75d989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 150\n",
      "Number of features: 4\n",
      "Number of classes: 3\n",
      "Feature names: ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
      "Class distribution:\n",
      "  Class 0: 50 samples\n",
      "  Class 1: 50 samples\n",
      "  Class 2: 50 samples\n",
      "10 iterations\n",
      "MAE: 0.13142610970996216\n",
      "NMI: 0.7419116631817836\n"
     ]
    }
   ],
   "source": [
    "data, labels = load_iris_data()\n",
    "print_iris_info(data, labels)\n",
    "# Your code here\n",
    "r = 3\n",
    "l = 10\n",
    "\n",
    "X_init = init_centroids_greedy_pp(data, r, l)\n",
    "X_final, Y_final = kmeans(data, r, X_init)\n",
    "\n",
    "mean_approx_error = RSS(data, X_final, Y_final) / (data.shape[0] * data.shape[1])\n",
    "\n",
    "predicted_labels = np.argmax(Y_final, axis=1)\n",
    "nmi = normalized_mutual_info_score(labels, predicted_labels)\n",
    "\n",
    "print(f\"MAE: {mean_approx_error}\")\n",
    "print(f\"NMI: {nmi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNuxVP-2sxxM"
   },
   "source": [
    "# 6A. Image Classification With Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFCoKoqZszY1"
   },
   "outputs": [],
   "source": [
    "# This is the helper class that consists of parameters for training and visualization.\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration parameters for the model and training.\"\"\"\n",
    "\n",
    "    # Model architecture\n",
    "    embedding_dim: int = 2\n",
    "    num_classes: int = 10\n",
    "\n",
    "    # Training hyperparameters\n",
    "    learning_rate: float = 0.9\n",
    "    momentum: float = 0.9\n",
    "    weight_decay: float = 5e-4\n",
    "    batch_size: int = 128\n",
    "    epochs: int = 5\n",
    "    dropout_rate_1: float = 0.9\n",
    "    dropout_rate_2: float = 0.9\n",
    "\n",
    "    # Visualization\n",
    "    viz_samples: int = 100\n",
    "    viz_zoom: float = 0.7\n",
    "    grid_resolution: float = 0.1\n",
    "\n",
    "    # Paths\n",
    "    checkpoint_dir: Path = Path(\"checkpoint\")\n",
    "    model_filename: str = \"embedding_model.pth\"\n",
    "\n",
    "    @property\n",
    "    def device(self) -> str:\n",
    "        \"\"\"Get the appropriate device for computation.\"\"\"\n",
    "        return 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lq7FlX69vqzZ"
   },
   "outputs": [],
   "source": [
    "# This is the implementation of the model architecture.\n",
    "# You DO NOT NEED TO CHANGE the code, you only need to print out the output shape.\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with skip connections and grouped convolutions.\n",
    "\n",
    "    Implements: output = input + F(input)\n",
    "    where F is a residual function composed of BatchNorm→ReLU→Conv layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, groups: int = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        groups = min(groups, min(in_channels, out_channels))\n",
    "\n",
    "        # Main convolution pathway\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                              padding=\"same\", groups=groups)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size,\n",
    "                              padding=\"same\", groups=min(groups, out_channels))\n",
    "\n",
    "        # Skip connection (identity or dimension adjustment)\n",
    "        self.skip_connection = (\n",
    "            nn.Identity() if in_channels == out_channels\n",
    "            else nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=\"same\")\n",
    "        )\n",
    "\n",
    "        # Pre-activation normalization layers\n",
    "        self.norm1 = nn.BatchNorm2d(in_channels)\n",
    "        self.norm2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass implementing residual connection.\"\"\"\n",
    "        identity = self.skip_connection(x)\n",
    "\n",
    "        # Residual pathway: BatchNorm → ReLU → Conv → BatchNorm → ReLU → Conv\n",
    "        out = self.conv1(self.relu(self.norm1(x)))\n",
    "        out = self.conv2(self.relu(self.norm2(out)))\n",
    "\n",
    "        return identity + out\n",
    "\n",
    "\n",
    "class EmbeddingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN that maps input images to low-dimensional embedding space.\n",
    "\n",
    "    Uses global average pooling instead of flattening to reduce overfitting\n",
    "    and make the model robust to different input sizes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int, dropout_rate_1: float, dropout_rate_2: float):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initial feature extraction\n",
    "        self.initial_conv = nn.Conv2d(1, 32, kernel_size=5, padding=\"same\")\n",
    "        self.initial_norm = nn.BatchNorm2d(32)\n",
    "\n",
    "        # First residual block set (32 channels, groups=2)\n",
    "        self.res_block1 = ResidualBlock(32, 32, kernel_size=3, groups=2)\n",
    "        self.res_block2 = ResidualBlock(32, 32, kernel_size=3, groups=2)\n",
    "\n",
    "        # Spatial downsampling\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.norm_after_pool = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Second residual block set (64 channels, groups=4)\n",
    "        self.res_block3 = ResidualBlock(32, 64, kernel_size=3, groups=4)\n",
    "        self.res_block4 = ResidualBlock(64, 64, kernel_size=3, groups=4)\n",
    "\n",
    "        # Final processing\n",
    "        self.final_norm = nn.BatchNorm1d(64)\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, embedding_dim)\n",
    "\n",
    "        # Regularization\n",
    "        self.dropout1 = nn.Dropout(dropout_rate_1)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate_2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass mapping images to embedding space.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, 1, 28, 28)\n",
    "\n",
    "        Returns:\n",
    "            Embedding tensor of shape (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        out = F.relu(self.initial_norm(self.initial_conv(x)))\n",
    "\n",
    "        # out.shape = ? (tensor shape 1)\n",
    "\n",
    "        # First round of residual blocks\n",
    "        out = self.res_block2(self.res_block1(out))\n",
    "\n",
    "        # out.shape = ? (tensor shape 2)\n",
    "\n",
    "        # Pooling\n",
    "        out = self.norm_after_pool(self.pool(out))\n",
    "\n",
    "        # out.shape = ? (tensor shape 3)\n",
    "\n",
    "        # Second round of residual blocks\n",
    "        out = self.res_block4(self.res_block3(out))\n",
    "\n",
    "        # out.shape = ? (tensor shape 4)\n",
    "\n",
    "        # Global average pooling\n",
    "        out = torch.mean(out, dim=(-1, -2))\n",
    "        out = self.final_norm(out)\n",
    "\n",
    "        # out.shape = ? (tensor shape 5)\n",
    "\n",
    "        # Map to embedding space\n",
    "        out = self.dropout1(out)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    \"\"\"Complete model combining embedding network with classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int, num_classes: int, config: Config):\n",
    "        super().__init__()\n",
    "        self.embedding_net = EmbeddingNetwork(\n",
    "            embedding_dim, config.dropout_rate_1, config.dropout_rate_2\n",
    "        )\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes, bias=True)\n",
    "\n",
    "        nn.init.normal_(self.classifier.weight, 0, 0.01)\n",
    "        nn.init.constant_(self.classifier.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for training and evaluation.\"\"\"\n",
    "        embeddings = self.embedding_net(x)\n",
    "        return self.classifier(embeddings)\n",
    "\n",
    "    def get_embeddings(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Extract embeddings for visualization.\"\"\"\n",
    "        return self.embedding_net(x)\n",
    "\n",
    "    def get_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get class probabilities for confidence visualization.\"\"\"\n",
    "        embeddings = self.embedding_net(x)\n",
    "        return F.softmax(self.classifier(embeddings), dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "fdfwcNo7u5cH",
    "outputId": "59ff2653-e07c-452b-826e-b44214a378e5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2874932927.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# You DO NOT NEED to CHANGE anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_data_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m      6\u001b[0m     \u001b[0mCreate\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtest\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Config' is not defined"
     ]
    }
   ],
   "source": [
    "# This is the helper code that downloads and calls the datasets and applies simple transformations.\n",
    "# You DO NOT NEED to CHANGE anything.\n",
    "\n",
    "def create_data_loaders(dataset_class, config: Config) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Create training and test data loaders.\n",
    "\n",
    "    Args:\n",
    "        dataset_class: torchvision dataset class (MNIST or FashionMNIST)\n",
    "        config: Configuration object\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_loader, test_loader)\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # MNIST standard values\n",
    "    ])\n",
    "\n",
    "    # Training data\n",
    "    train_dataset = dataset_class(root='./data', train=True, download=True, transform=transform)\n",
    "    if config.num_classes < 10:\n",
    "        mask = train_dataset.targets < config.num_classes\n",
    "        train_dataset.targets = train_dataset.targets[mask]\n",
    "        train_dataset.data = train_dataset.data[mask]\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2\n",
    "    )\n",
    "\n",
    "    # Test data\n",
    "    test_dataset = dataset_class(root='./data', train=False, download=True, transform=transform)\n",
    "    if config.num_classes < 10:\n",
    "        mask = test_dataset.targets < config.num_classes\n",
    "        test_dataset.targets = test_dataset.targets[mask]\n",
    "        test_dataset.data = test_dataset.data[mask]\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "YiPKhQNRx4zA"
   },
   "outputs": [],
   "source": [
    "# This is the helper code for evaluating and calculating the metrics.\n",
    "# You DO NOT NEED to CHANGE anything.\n",
    "\n",
    "@dataclass\n",
    "class EpochMetrics:\n",
    "    \"\"\"Container for epoch training/evaluation metrics.\"\"\"\n",
    "    accuracy: float\n",
    "    avg_confidence: float\n",
    "    avg_loss: float\n",
    "    total_samples: int\n",
    "    elapsed_time: Optional[float] = None\n",
    "\n",
    "\n",
    "def compute_batch_metrics(logits: torch.Tensor, targets: torch.Tensor, loss: torch.Tensor) -> Tuple[int, float, int]:\n",
    "    \"\"\"\n",
    "    Compute metrics for a single batch.\n",
    "\n",
    "    Args:\n",
    "        logits: Model output logits\n",
    "        targets: Ground truth labels\n",
    "        loss: Computed loss for the batch\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (correct_predictions, total_confidence, batch_size)\n",
    "    \"\"\"\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    confidences, predictions = probabilities.max(1)\n",
    "\n",
    "    correct_predictions = predictions.eq(targets).sum().item()\n",
    "    total_confidence = confidences.sum().item()\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    return correct_predictions, total_confidence, batch_size\n",
    "\n",
    "\n",
    "def run_epoch(\n",
    "    model: nn.Module,\n",
    "    criterion,\n",
    "    data_loader: DataLoader,\n",
    "    device: str,\n",
    "    optimizer=None,\n",
    "    is_training: bool = True\n",
    ") -> EpochMetrics:\n",
    "    \"\"\"\n",
    "    Run one epoch of training or evaluation.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        criterion: Loss function\n",
    "        data_loader: Data loader\n",
    "        device: Device to run on\n",
    "        optimizer: Optimizer (required if is_training=True)\n",
    "        is_training: Whether to run in training mode\n",
    "\n",
    "    Returns:\n",
    "        EpochMetrics containing all computed metrics\n",
    "    \"\"\"\n",
    "    if is_training:\n",
    "        if optimizer is None:\n",
    "            raise ValueError(\"Optimizer required for training mode\")\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_confidence = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    context_manager = torch.no_grad() if not is_training else torch.enable_grad()\n",
    "\n",
    "    with context_manager:\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if is_training:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                print(\"Warning: NaN loss detected\")\n",
    "\n",
    "            if is_training:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Compute batch metrics (always without gradients for metrics)\n",
    "            with torch.no_grad():\n",
    "                batch_correct, batch_confidence, batch_size = compute_batch_metrics(logits, targets, loss)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_correct += batch_correct\n",
    "                total_confidence += batch_confidence\n",
    "                total_samples += batch_size\n",
    "\n",
    "    # Calculate final metrics\n",
    "    if total_samples == 0:\n",
    "        print(\"Warning: No samples processed\")\n",
    "        return EpochMetrics(0, 0, float('inf'), 0, time.time() - start_time)\n",
    "\n",
    "    accuracy = 100.0 * total_correct / total_samples\n",
    "    avg_confidence = 100.0 * total_confidence / total_samples\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    return EpochMetrics(\n",
    "        accuracy=accuracy,\n",
    "        avg_confidence=avg_confidence,\n",
    "        avg_loss=avg_loss,\n",
    "        total_samples=total_samples,\n",
    "        elapsed_time=elapsed_time\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "AkYO9Xdgx41J"
   },
   "outputs": [],
   "source": [
    "# This is the helper function for training.\n",
    "# You DO NOT NEED to CHANGE anything.\n",
    "\n",
    "def train_epoch(model: nn.Module, criterion, optimizer, data_loader: DataLoader, device: str) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Train model for one epoch.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (accuracy, average_confidence)\n",
    "    \"\"\"\n",
    "    metrics = run_epoch(model, criterion, data_loader, device, optimizer, is_training=True)\n",
    "\n",
    "    print(f'Train - Loss: {metrics.avg_loss:.3f} | '\n",
    "          f'Acc: {metrics.accuracy:.3f}% ({int(metrics.accuracy * metrics.total_samples / 100)}/{metrics.total_samples}) | '\n",
    "          f'Conf: {metrics.avg_confidence:.2f}% | Time: {metrics.elapsed_time:.2f}s')\n",
    "\n",
    "    return metrics.accuracy, metrics.avg_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CR6BXPEbx43S"
   },
   "outputs": [],
   "source": [
    "# This is the helper function for testing.\n",
    "# You DO NOT NEED to CHANGE anything.\n",
    "\n",
    "def evaluate_model(model: nn.Module, criterion, data_loader: DataLoader, device: str) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate model on test data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (accuracy, average_confidence)\n",
    "    \"\"\"\n",
    "    metrics = run_epoch(model, criterion, data_loader, device, optimizer=None, is_training=False)\n",
    "\n",
    "    print(f'Test  - Loss: {metrics.avg_loss:.3f} | '\n",
    "          f'Acc: {metrics.accuracy:.3f}% ({int(metrics.accuracy * metrics.total_samples / 100)}/{metrics.total_samples}) | '\n",
    "          f'Conf: {metrics.avg_confidence:.2f}%')\n",
    "\n",
    "    return metrics.accuracy, metrics.avg_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7Hhrf_Tux45V"
   },
   "outputs": [],
   "source": [
    "# This is the helper function for saving and then loading the saved model.\n",
    "# You DO NOT NEED to CHANGE anything.\n",
    "\n",
    "def save_model(model: nn.Module, accuracy: float, config: Config) -> None:\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    config.checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'accuracy': accuracy,\n",
    "        'config': {\n",
    "            'embedding_dim': config.embedding_dim,\n",
    "            'num_classes': config.num_classes,\n",
    "            'dropout_rate_1': config.dropout_rate_1,\n",
    "            'dropout_rate_2': config.dropout_rate_2,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    save_path = config.checkpoint_dir / config.model_filename\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "\n",
    "def load_model(config: Config) -> EmbeddingClassifier:\n",
    "    \"\"\"Load model from checkpoint.\"\"\"\n",
    "    load_path = config.checkpoint_dir / config.model_filename\n",
    "\n",
    "    model = EmbeddingClassifier(config.embedding_dim, config.num_classes, config)\n",
    "    checkpoint = torch.load(load_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Model loaded from {load_path}\")\n",
    "    print(f\"Loaded model accuracy: {checkpoint['accuracy']:.2f}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "iICob85ox47s"
   },
   "outputs": [],
   "source": [
    "# This is the helper function for visualizing the results for the Open-Answer Question.\n",
    "# You DO NOT NEED to CHANGE anything.\n",
    "def plot_decision_boundary(\n",
    "    model: EmbeddingClassifier,\n",
    "    bounds: Tuple[float, float, float, float],\n",
    "    config: Config,\n",
    "    show_classes: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot decision boundary or confidence map in embedding space.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        bounds: (x_min, x_max, y_min, y_max) for plot region\n",
    "        config: Configuration object\n",
    "        show_classes: If True, show class assignments; if False, show confidence\n",
    "    \"\"\"\n",
    "    x_min, x_max, y_min, y_max = bounds\n",
    "\n",
    "    if not all(np.isfinite([x_min, x_max, y_min, y_max])):\n",
    "        print(\"Warning: Invalid bounds detected, using default range\")\n",
    "        x_min, x_max, y_min, y_max = -10, 10, -10, 10\n",
    "\n",
    "    if x_max <= x_min:\n",
    "        x_max = x_min + 10\n",
    "    if y_max <= y_min:\n",
    "        y_max = y_min + 10\n",
    "\n",
    "    x = np.arange(x_min, x_max, config.grid_resolution, dtype=np.float32)\n",
    "    y = np.arange(y_min, y_max, config.grid_resolution, dtype=np.float32)\n",
    "\n",
    "    if len(x) == 0 or len(y) == 0:\n",
    "        print(\"Warning: Empty grid, adjusting resolution\")\n",
    "        x = np.linspace(x_min, x_max, 50, dtype=np.float32)\n",
    "        y = np.linspace(y_min, y_max, 50, dtype=np.float32)\n",
    "\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "    # Create grid points for evaluation\n",
    "    grid_points = torch.from_numpy(\n",
    "        np.array([xx.ravel(), yy.ravel()]).T\n",
    "    ).float().to(config.device)\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        probabilities = torch.softmax(model.classifier(grid_points), dim=1)\n",
    "        probabilities = probabilities.cpu().numpy()\n",
    "\n",
    "    # Reshape for contour plotting\n",
    "    if show_classes:\n",
    "        class_assignments = probabilities.argmax(axis=1).reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, class_assignments, levels=config.num_classes, cmap='tab10', alpha=0.7)\n",
    "        plt.colorbar(label='Predicted Class')\n",
    "    else:\n",
    "        confidence_map = probabilities.max(axis=1).reshape(xx.shape)\n",
    "        contour = plt.contourf(xx, yy, confidence_map, levels=20, cmap='viridis', alpha=0.7)\n",
    "        plt.clim(0, 1)\n",
    "        plt.colorbar(contour, label='Max Confidence')\n",
    "\n",
    "    plt.axis('equal')\n",
    "\n",
    "\n",
    "def scatter_images_on_embeddings(\n",
    "    images: torch.Tensor,\n",
    "    embeddings: torch.Tensor,\n",
    "    config: Config\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Scatter actual images at their embedding coordinates.\n",
    "\n",
    "    Args:\n",
    "        images: Input images tensor\n",
    "        embeddings: Corresponding embedding coordinates\n",
    "        config: Configuration object\n",
    "    \"\"\"\n",
    "    num_samples = min(images.shape[0], config.viz_samples)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image = images[i].squeeze().cpu().numpy()\n",
    "        embedding_pos = (embeddings[i, 0].item(), embeddings[i, 1].item())\n",
    "\n",
    "        if not all(np.isfinite(embedding_pos)):\n",
    "            continue\n",
    "\n",
    "        offset_image = OffsetImage(image, cmap=\"gray\", zoom=config.viz_zoom)\n",
    "        annotation_box = AnnotationBbox(\n",
    "            offset_image, embedding_pos, xycoords='data', frameon=False, alpha=0.7\n",
    "        )\n",
    "        plt.gca().add_artist(annotation_box)\n",
    "\n",
    "\n",
    "def visualize_embedding_space(\n",
    "    model: EmbeddingClassifier,\n",
    "    data_loader: DataLoader,\n",
    "    config: Config,\n",
    "    title: str = \"Embedding Space Visualization\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of embedding space.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        data_loader: Data loader for visualization\n",
    "        config: Configuration object\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Get batch of data and embeddings\n",
    "    inputs, _ = next(iter(data_loader))\n",
    "    inputs = inputs.to(config.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_embeddings(inputs).cpu()\n",
    "\n",
    "    valid_embeddings = embeddings[torch.isfinite(embeddings).all(dim=1)]\n",
    "\n",
    "    if len(valid_embeddings) == 0:\n",
    "        print(\"Warning: No valid embeddings found, using default bounds\")\n",
    "        bounds = (-10, 10, -10, 10)\n",
    "    else:\n",
    "        margin = 3\n",
    "        x_vals = valid_embeddings[:, 0]\n",
    "        y_vals = valid_embeddings[:, 1]\n",
    "\n",
    "        bounds = (\n",
    "            float(x_vals.min() - margin),\n",
    "            float(x_vals.max() + margin),\n",
    "            float(y_vals.min() - margin),\n",
    "            float(y_vals.max() + margin)\n",
    "        )\n",
    "\n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plot_decision_boundary(model, bounds, config)\n",
    "    scatter_images_on_embeddings(inputs.cpu(), embeddings, config)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Embedding Dimension 1')\n",
    "    plt.ylabel('Embedding Dimension 2')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjCW-USzsziZ"
   },
   "source": [
    "# 6b. Image Classification - Open Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbN1RRP2n0E5"
   },
   "outputs": [],
   "source": [
    "# PLEASE CHANGE the configuration below to obtain better results.\n",
    "\n",
    "\"\"\"Main training and evaluation pipeline.\"\"\"\n",
    "\n",
    "config = Config(\n",
    "    epochs = 10,  # Make sure performance converges\n",
    "\n",
    ")\n",
    "\n",
    "print(\"CNN Embedding Space Learning\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Using device: {config.device}\")\n",
    "\n",
    "# Create data loaders\n",
    "print(\"\\nPreparing MNIST data...\")\n",
    "mnist_train_loader, mnist_test_loader = create_data_loaders(datasets.MNIST, config)\n",
    "\n",
    "# Create and setup model\n",
    "print(\"Building model...\")\n",
    "model = EmbeddingClassifier(config.embedding_dim, config.num_classes, config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    momentum=config.momentum,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nTraining...\")\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{config.epochs}:')\n",
    "    train_epoch(model, criterion, optimizer, mnist_train_loader, config.device)\n",
    "    test_acc, _ = evaluate_model(model, criterion, mnist_test_loader, config.device)\n",
    "\n",
    "    if test_acc > best_accuracy:\n",
    "        best_accuracy = test_acc\n",
    "\n",
    "# Save model\n",
    "save_model(model, best_accuracy, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKL6fs0KuHMF"
   },
   "outputs": [],
   "source": [
    "# Visualize MNIST embeddings\n",
    "# You DO NOT NEED to CHANGE anything.\n",
    "\n",
    "print(\"\\nVisualizing MNIST embeddings...\")\n",
    "try:\n",
    "    visualize_embedding_space(model, mnist_test_loader, config, \"MNIST Embedding Space\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Visualization error: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nResults Summary:\")\n",
    "print(f\"MNIST Test Accuracy: {best_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
