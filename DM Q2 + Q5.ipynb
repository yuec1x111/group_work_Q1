{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b36f2a",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "The assignment is accompanied by a setup notebook that contains boilerplate code for each exercise. Some of the answers depend on a particular dataset split or seed, so consult the boilerplate code for specifics even if you would like to program everything from scratch.\n",
    "\n",
    "## Software Libraries\n",
    "\n",
    "For this assignment, we strongly encourage you to use NumPy for your implementations. While scikit-learn provides convenient implementations of many algorithms, we recommend implementing the core algorithms from scratch using NumPy for several important reasons:\n",
    "\n",
    "**Educational Value**: Implementing algorithms from scratch deepens your understanding of the underlying mathematics and computational principles. You'll gain insight into the inner workings that you might miss when using high-level libraries.\n",
    "\n",
    "**Control and Transparency**: NumPy gives you precise control over your implementations without the hidden complexity that can exist in scikit-learn's optimized code. This transparency is crucial for debugging and understanding exactly what your code is doing.\n",
    "\n",
    "**Avoiding Implementation Pitfalls**: If you do choose to use scikit-learn, please be extremely careful. The library contains many implementation details, optimizations, and default parameters that can significantly affect your results. These details are not always obvious from the documentation and can lead to incorrect conclusions if not properly understood.\n",
    "\n",
    "If you decide to use scikit-learn despite this recommendation, please thoroughly read the documentation and understand all parameters and their effects.\n",
    "\n",
    "## A Note On Using Language Models\n",
    "As a student in the age of AI, you have without a doubt used language models to study course material, answer questions or even solve assignments for you. Using other sources than the course material to learn is valuable indeed, but a word of caution:\n",
    "\n",
    "The point of these exercises is for you to develop as an engineer and as a scientist. Using language models to directly answer the question might get you through the assignment, but it is liable to damage your development in the long run. In the end **someone needs to understand what the model is talking about in order to validate the outcomes.**\n",
    "\n",
    "It is quite likely you will not end up designing gradient descent algorithms for a living. But do not confuse the specifics of this problem set with the broader educational value of **solving a difficult problem yourself from start to finish**.\n",
    "\n",
    "Education is about more than gaining skills. It is also about a work ethic, a way of approaching problems, scientific rigor, managing your thought process, using the tools at your disposal and recognizing where your strengths and weaknesses lie. Using language models heedlessly runs the risk of depriving you of the opportunity to struggle through that process.\n",
    "\n",
    "We can only ask you to adopt more responsibility for your education yourself. Consider not denying yourself the challenge and pressure necessary for growth, and instead use the tools you have to deepen your understanding. In the end, the responsibility is yours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f318ae3-2456-4d32-a784-1e42dfd157ee",
   "metadata": {},
   "source": [
    "# 1. Linear Algebra for Datamining and Machine Learning\n",
    "\n",
    "This assignment focuses on linear algebra concepts essential for understanding \n",
    "Principal Component Analysis (PCA), covariance analysis, and feature transformations.\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{m\\times p}$ be a centered data matrix where each row \n",
    "represents an observation and each column represents a feature. The centering \n",
    "condition means each column has zero mean: $\\sum_{i=1}^m X_{i,j} = 0$ for all $j$.\n",
    "\n",
    "## 1a. Eigenvalues and Total Variance\n",
    "Given the eigendecomposition of the sample covariance matrix\n",
    "$$\n",
    "C = \\frac{1}{m-1} X^\\top X = Q \\Lambda Q^\\top\n",
    "$$\n",
    "where $Q$ contains the eigenvectors and $\\Lambda$ is diagonal with eigenvalues \n",
    "$\\lambda_1, \\lambda_2, \\ldots, \\lambda_p$, the total variance in the dataset equals:\n",
    "$$\n",
    "\\text{Total Sample Variance} = \\operatorname{tr}(C) = \\frac{1}{m-1}\\operatorname{tr}(X^\\top X) = \\lambda_{max}\n",
    "$$\n",
    "\n",
    "○ False  \n",
    "○ True\n",
    "\n",
    "## 1b. Covariance for Centered Data\n",
    "For a centered data matrix $ X $, the sample covariance between features $ j $ and $ k $ can be computed as:\n",
    "\n",
    "$$\n",
    "\\operatorname{Cov}(j,k) = \\frac{1}{m-1} \\, X_{(:,j)}^\\top X_{(:,k)}\n",
    "$$\n",
    "\n",
    "where $ X_{(:,j)} $ denotes the $ j $-th column of $ X $.\n",
    "\n",
    "○ False  \n",
    "○ True\n",
    "\n",
    "## 1c. Covariance Under Linear Transformations\n",
    "\n",
    "If we apply a linear transformation\n",
    "\n",
    "$$\n",
    "Y = X W\n",
    "$$\n",
    "\n",
    "where $ W \\in \\mathbb{R}^{p\\times q} $ is a transformation matrix, then the covariance matrix of the transformed features is:\n",
    "\n",
    "$$\n",
    "C_Y = W^\\top C_X W\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "C_X = \\frac{X^\\top X}{m-1}\n",
    "$$\n",
    "\n",
    "is the covariance matrix of $ X $.\n",
    "\n",
    "○ False  \n",
    "○ True\n",
    "\n",
    "## 1d. Matrix Norms and Trace Properties\n",
    "For any matrix $ A \\in \\mathbb{R}^{n\\times d} $, the following relationship holds between row-wise and column-wise squared norms:\n",
    "\n",
    "$$\n",
    "\\operatorname{tr}(A A^\\top) = \\sum_{i=1}^n \\|A_{(i,:)}\\|^2 = \\sum_{j=1}^d \\|A_{(:,j)}\\|^2\n",
    "$$\n",
    "\n",
    "○ False  \n",
    "○ True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44259ed6-3c66-44f1-a4a7-43b5028931a6",
   "metadata": {},
   "source": [
    "# 2. Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea598c71",
   "metadata": {},
   "source": [
    "In this exercise, you'll implement and explore different data normalization techniques, fundamental preprocessing steps in machine learning and data analysis. Raw data often comes with features on vastly different scales. Imagine comparing house prices in dollars with number of bedrooms, or pixel intensities (0-255) with age in years. Without proper scaling, algorithms can be biased toward features with larger numerical ranges, leading to poor performance.\n",
    "\n",
    "Data normalization addresses this by transforming features to comparable scales while preserving the underlying relationships in the data. Different scaling methods make different assumptions about the data distribution and have varying robustness to outliers:\n",
    "\n",
    "- **Min-Max Scaling** transforms features to a fixed range (typically [0,1]), preserving the original distribution shape but being sensitive to outliers\n",
    "- **Standardization (Z-score)** centers data around zero with unit variance, assuming normally distributed data\n",
    "- **Robust Scaling** uses median and interquartile range, making it less sensitive to extreme values\n",
    "\n",
    "We will work with a small product sales dataset that contains information about five products.\n",
    "Each row corresponds to a product (ID 0–4), and each column is a feature:\n",
    "\n",
    "- *F1*: UnitsSold → Number of units sold\n",
    "- *F2*: Revenue → Total revenue generated (in euros)\n",
    "- *F3*: Returns → Number of returned items\n",
    "\n",
    "The dataset is given by:\n",
    "```\n",
    "ID  F1   F2     F3\n",
    "0   50   2000   5 \n",
    "1   70   2500   7 \n",
    "2   65   2100   6 \n",
    "3   500  20000  40\n",
    "4   60   2200   5 \n",
    "```\n",
    "\n",
    "This dataset is deliberately chosen because the scales are very different:\n",
    "- UnitsSold ranges from 50 to 500 (with an outlier at 500).\n",
    "- Revenue ranges from 2000 to 20000, much larger values than the others.\n",
    "- Returns are small numbers (5–40).\n",
    "\n",
    "You will implement and apply different scaling strategies to understand how each transformation affects the data distribution and individual values.\n",
    "\n",
    "You are asked to:\n",
    "\n",
    "- Implement a function that applies **min-max scaling** to transform features to the range [0,1]:\n",
    "  ```python\n",
    "  def minmax_scale(data: list) -> list:\n",
    "  ```\n",
    "  using the transformation:\n",
    "  $$\n",
    "  x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}\n",
    "  $$\n",
    "\n",
    "- Implement a function that applies **standardization** (z-score normalization):\n",
    "  ```python\n",
    "  def standardize(data: list) -> list:\n",
    "  ```\n",
    "  using the transformation:\n",
    "  $$\n",
    "  x_{standardized} = \\frac{x - \\mu}{\\sigma}\n",
    "  $$\n",
    "  where $\\mu$ is the mean and $\\sigma$ is the standard deviation.\n",
    "\n",
    "- Implement a function that applies **robust scaling**:\n",
    "  ```python\n",
    "  def robust_scale(data: list) -> list:\n",
    "  ```\n",
    "  using the transformation:\n",
    "  $$\n",
    "  x_{robust} = \\frac{x - \\text{median}(x)}{\\text{IQR}(x)}\n",
    "  $$\n",
    "  where IQR is the interquartile range (Q3 - Q1).\n",
    "\n",
    "**All final values should be rounded to two decimal places.**\n",
    "\n",
    "For each scaling method, you should:\n",
    "- Apply the transformation to the specified feature column\n",
    "- Report the transformed values\n",
    "- Analyze how the scaling affects the data distribution\n",
    "\n",
    "## 2a. Min-Max Scaling\n",
    "\n",
    "Apply min-max scaling to feature **F1** and report the complete transformed column:\n",
    "\n",
    "**Processed values for F1 with min-max scaling:**\n",
    "$$\n",
    "F1_{minmax} = [?, ?, ?, ?, ?]\n",
    "$$\n",
    "\n",
    "## 2b. Standardization\n",
    "\n",
    "Apply standardization to feature **F2** and report the specific transformed value for ID 4:\n",
    "\n",
    "**Processed value for ID 4's F2 after standardization:**\n",
    "$$\n",
    "F2_{standardized}[ID=4] = ?\n",
    "$$\n",
    "\n",
    "## 2c. Robust Scaling\n",
    "\n",
    "Apply robust scaling to feature **F3** and report the specific transformed value for ID 3:\n",
    "\n",
    "**Processed value for ID 3's F3 after robust scaling:**\n",
    "$$\n",
    "F3_{robust}[ID=3] = ?\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f86857c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID_4: -0.5\n",
      "ID_3: 17.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F1_MinMax</th>\n",
       "      <th>F2_Standardized</th>\n",
       "      <th>F3_Robust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>2000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70</td>\n",
       "      <td>2500</td>\n",
       "      <td>7</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65</td>\n",
       "      <td>2100</td>\n",
       "      <td>6</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500</td>\n",
       "      <td>20000</td>\n",
       "      <td>40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>2200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    F1     F2  F3  F1_MinMax  F2_Standardized  F3_Robust\n",
       "0   50   2000   5       0.00            -0.53       -0.5\n",
       "1   70   2500   7       0.04            -0.46        0.5\n",
       "2   65   2100   6       0.03            -0.51        0.0\n",
       "3  500  20000  40       1.00             2.00       17.0\n",
       "4   60   2200   5       0.02            -0.50       -0.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "dataset = pd.DataFrame({'F1': [50, 70, 65, 500, 60], 'F2': [2000, 2500, 2100, 20000, 2200], 'F3': [5, 7, 6, 40, 5]})\n",
    "dataset.head(10)\n",
    "\n",
    "# 2a. Min-Max Scaling F1\n",
    "def min_max_scale(data: list) -> list:\n",
    "    x_min = min(data)\n",
    "    x_max = max(data)\n",
    "    x_scaled = [round((x-x_min)/(x_max-x_min), 2) for x in data]\n",
    "    return x_scaled\n",
    "\n",
    "dataset['F1_MinMax'] = min_max_scale(dataset['F1'].tolist())\n",
    "\n",
    "# 2b. Standardization F2\n",
    "def standardize(data: list) -> list:\n",
    "    mean = np.mean(data)\n",
    "    sd = np.std(data)\n",
    "    x_standardized = [round((x-mean)/sd, 2) for x in data]\n",
    "    return x_standardized\n",
    "dataset['F2_Standardized'] = standardize(dataset['F2'].tolist())\n",
    "ID_4 = dataset['F2_Standardized'][4]\n",
    "print(f'ID_4: {ID_4}')\n",
    "\n",
    "# 2c. Robust Scaling F3\n",
    "def robust_scale(data: list) -> list:\n",
    "    median = np.median(data)\n",
    "    iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "    x_robust = [round((x-median)/iqr, 2) for x in data]\n",
    "    return x_robust\n",
    "dataset['F3_Robust'] = robust_scale(dataset['F3'].tolist())\n",
    "ID_3 = dataset['F3_Robust'][3]\n",
    "print(f'ID_3: {ID_3}')\n",
    "\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d678e7b7cfdedc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. Gradient Descent\n",
    "\n",
    "In this exercise, you'll implement and explore gradient descent, a cornerstone of modern optimization. The method is simple in idea but powerful in application: we iteratively update our variables in the direction that most rapidly decreases the function, based on the **negative gradient**. While coordinate descent updates one variable at a time, gradient descent moves in the direction of steepest descent across all coordinates simultaneously. This often leads to:\n",
    "\n",
    "- Faster convergence when variables are tightly coupled or when the function isn’t naturally separable,\n",
    "- More direct progress toward a minimum, especially in smooth, well-behaved functions,\n",
    "- Better behavior in high-dimensional settings where dependencies across variables are strong.\n",
    "\n",
    "You will be exploring the concept by taking a look at minimizing **Himmelblau’s function**, a classical example in optimization with multiple local minima. It is often used to study the behavior of optimization algorithms such as gradient descent in non-convex settings.\n",
    "\n",
    "The function is given by\n",
    "$$\n",
    "f(u, v) = (u^2 + v - 11)^2 + (u + v^2 - 7)^2\n",
    "$$\n",
    "where $u, v \\in \\mathbb{R}$.\n",
    "\n",
    "You will use the **gradient descent** algorithm to find a minimum of $f$. Specifically, you will investigate how different **step-size strategies** and **initial points** affect convergence.\n",
    "\n",
    "You are asked to:\n",
    "\n",
    "- Implement a function that takes a point $(u, v)$ and returns the gradient $\\nabla f(u, v)$ at that point.\n",
    "- Implement a function\n",
    "  ```python\n",
    "  def gradient_descent(f, grad_f, eta, u0, v0, max_iter=100) -> tuple[list, list]:\n",
    "  ```\n",
    "  that performs the update rule:\n",
    "  $$\n",
    "  x_{t+1} \\leftarrow x_t - \\eta(t) \\nabla f(x_t)\n",
    "  $$\n",
    "  where input $x_t$ is given by `x_t = (u_t, v_t)` and `eta(t)` defines the python method that returns the step size at iteration $t$. It is useful to make it return both the path and the computed values at each step.\n",
    "- Using this setup, run 100 steps of gradient descent starting at $(u_0, v_0) = (4, -5)$ and evaluate different step-size strategies.\n",
    "- Evaluate different starting points.\n",
    "\n",
    "For each of the following strategies, report:\n",
    "\n",
    "- The final function value:\n",
    "  $$\n",
    "  f(u_{100}, v_{100}) =\n",
    "  $$\n",
    "- The best (lowest) value reached during training:\n",
    "  $$\n",
    "  \\min_{1 \\leq t \\leq 100} f(u_t, v_t) =\n",
    "  $$\n",
    "\n",
    "## 3a. Constant Step Size\n",
    "\n",
    "Implement a constant step-size strategy $\\eta=c$:\n",
    "```python\n",
    "def eta_const(t,c=1e-3) -> float:\n",
    "```\n",
    "\n",
    "## 3b. Decreasing Step Size (Inverse Square Root)\n",
    "\n",
    "Implement a decreasing step size $\\eta=c/\\sqrt{t+1}$.\n",
    "```python\n",
    "def eta_sqrt(t,c=1e-3) -> float:\n",
    "```\n",
    "\n",
    "## 3c. Multi-Step Schedule\n",
    "\n",
    "Implement a **piecewise-decaying step size** that drops by a factor `c` at predefined milestones:\n",
    "\n",
    "$$\\begin{aligned} \\mathrm{eta\\_multistep(t, [20,50], c=0.1, eta\\_init=1)} = \\begin{cases} 1, & t<20\\\\ 0.1 & 20\\leq t<50\\\\ 0.01 & 50\\leq t \\end{cases} \\end{aligned}$$\n",
    "\n",
    "Implemented with the following interface:\n",
    "\n",
    "```python\n",
    "def eta_multistep(t, milestones=[20, 50], c=1e-4, eta_init=1e-3) -> float:\n",
    "```\n",
    "\n",
    "## 3d. Initialization\n",
    "\n",
    "Repeat the above experiments (e.g., using `eta_const`) with different starting points $(u_0, v_0)$:\n",
    "\n",
    "- $(-4, 0)$\n",
    "- $(0, 0)$\n",
    "- $(4, 0)$\n",
    "- $(0, 4)$\n",
    "- $(5, 5)$\n",
    "\n",
    "For each initialization:\n",
    "\n",
    "- Report the final point $(u_{100}, v_{100})$\n",
    "- Report the final function value\n",
    "- Optional (not graded): Plot the gradient descent trajectories of the different starting points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ff1607d260b97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4. Coordinate Descent\n",
    "\n",
    "In optimization, we often aim to find the global minimum of a function by solving the gradient equation analytically. However, this becomes impractical in many real-world settings. Functions may be too complex, too high-dimensional, or lack closed-form solutions. This is where **coordinate descent** provides an alternative.\n",
    "\n",
    "Coordinate descent updates one variable at a time, minimizing along each coordinate direction while holding the others fixed. This is particularly useful when:\n",
    "- The function is differentiable but hard to minimize jointly,\n",
    "- Partial updates are much easier to compute (analytically or numerically).\n",
    "\n",
    "In this assignment, you'll apply coordinate descent to the following function:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\exp(x_1 - 3x_2 + 3) + \\exp(3x_2 - 2x_3 - 2) + \\exp(2x_3 - x_1 + 2)\n",
    "$$\n",
    "with $x_1, x_2, x_3 \\in \\mathbb{R}$.\n",
    "\n",
    "This function is fully differentiable, and its coordinate-wise argmins are analytically tractable. In part $a$, you’ll derive the updates for each coordinate individually. In part $b$, you’ll use these updates to implement a full coordinate descent loop, stepping through each coordinate iteratively and observing the convergence behavior.\n",
    "\n",
    "## 4a. Analytically Computing Partial Gradients\n",
    "Implement for each coordinate $ x_i $, $( i \\in \\{1,2,3\\} )$ a function `argmin_xi(x)` that returns $\\arg\\min_{x_i} f(x)$, for each coordinate using the initial point $\\mathbf{x}_{t_0} = (4, 3, 2) $:\n",
    "\n",
    "- **$ \\arg\\min_{x_1} f({\\mathbf{x}_{t_0}}) = $**\n",
    "- **$ \\arg\\min_{x_2} f({\\mathbf{x}_{t_0}}) = $**\n",
    "- **$ \\arg\\min_{x_3} f({\\mathbf{x}_{t_0}}) = $**\n",
    "\n",
    "## 4b. Implementing The Coordinate Descent Loop\n",
    "Implement a function `coordinate_descent(f, argmin, x_t0, max_iter=25)` that performs `max_iter` coordinate descent steps, where:\n",
    "\n",
    "- `f` is the function to be minimized.\n",
    "- `argmin` is an array of the `argmin_xi` functions for each coordinate.\n",
    "- `x_t0` is the starting point (initialization).\n",
    "\n",
    "At iteration $ t $, update all coordinates in order:\n",
    "$$\n",
    "x_t[i] = \\text{argmin}[i](x_t)\n",
    "$$\n",
    "\n",
    "Using the initial point $\\mathbf{x}_{t_0} = (1, 20, 5)$, run your coordinate descent implementation and answer the following:\n",
    "\n",
    "- What are the final three coordinates (i.e. after the final step $t_n$)?\n",
    "    - **$ \\arg\\min_{x_1} f({\\mathbf{x}_{t_n}}) = $**\n",
    "    - **$ \\arg\\min_{x_2} f({\\mathbf{x}_{t_n}}) = $**\n",
    "    - **$ \\arg\\min_{x_3} f({\\mathbf{x}_{t_n}}) = $**\n",
    "- What is the value the coordinate descent converges to?\n",
    "  - $ f(\\mathbf{x}_{t_n}) = $\n",
    "- Optional (not graded): Use visualizations to validate your answer. Hint: A partial check you can perform is to see if you ended up in a local mimimum across dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f244521e3bb74275",
   "metadata": {},
   "source": [
    "# 5. Bias and Variance\n",
    "Consider the true regression function:\n",
    "$$\n",
    "f^*(x) = \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    " \n",
    "You might recognize this as the *sigmoid function*. Suppose we fit three regression models to different independent and identically distributed (i.i.d.) datasets sampled from the true regression function, $\\mathcal{D}_1, \\mathcal{D}_2, \\mathcal{D}_3$, resulting in the following models:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_{\\mathcal{D}_1}(x) &= 2x + 0.4 \\\\\n",
    "f_{\\mathcal{D}_2}(x) &= x + 0.1 \\\\\n",
    "f_{\\mathcal{D}_3}(x) &= 3x + 0.7\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For $ x_0 = 0 $, compute:\n",
    "1. The sample mean of the predictions $ f_{\\mathcal{D}_i}(x_0) $, $ i=1,2,3 $.\n",
    "2. The bias of the average predictor relative to $ f^*(x_0) $.\n",
    "3. The variance of the predictors at $ x_0 $.\n",
    "\n",
    "Then provide $bias^2$ and $variance$:\n",
    "\n",
    "$bias^2 = $\n",
    "\n",
    "$variance = $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f57c8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample mean: 0.4\n",
      "bias^2: 0.01\n",
      "variance: 0.06\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# 1. sample mean\n",
    "def f_D1(x: float) -> float:\n",
    "    return 2*x + 0.4\n",
    "\n",
    "def f_D2(x: float) -> float:\n",
    "    return x + 0.1\n",
    "\n",
    "def f_D3(x: float) -> float:\n",
    "    return 3*x + 0.7\n",
    "\n",
    "x_0 = 0\n",
    "\n",
    "sample_mean = round(np.mean([f_D1(x_0), f_D2(x_0), f_D3(x_0)]), 2)\n",
    "print(f'sample mean: {sample_mean}')\n",
    "\n",
    "# 2. bias\n",
    "def true_regression(x: float) -> float:\n",
    "    return 1/(1 + math.pow(math.e, -x))\n",
    "true_value = true_regression(x_0)\n",
    "bias_square = round((sample_mean - true_value)**2, 2)\n",
    "print(f'bias^2: {bias_square}')\n",
    "\n",
    "# 3. variance\n",
    "deviations = [(f_D1(x_0) - sample_mean)**2, (f_D2(x_0) - sample_mean)**2, (f_D3(x_0) - sample_mean)**2]\n",
    "variance = round(np.mean(deviations), 2)\n",
    "print(f'variance: {variance}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2c04aaaafa4b5",
   "metadata": {},
   "source": [
    "# 6. Polynomial Regression\n",
    "\n",
    "In this exercise, you’ll explore regression, a technique that is used to predict continuous outputs, i.e. real numbers, based on multiple inputs. You will implement a full modeling pipeline from raw data to fitted models. The focus is on building an intuitive and practical understanding of how regression models behave as they grow in complexity.\n",
    "\n",
    "You will use the California housing dataset. The accompanying Jupyter notebook includes instructions to load the data. Your task is to predict housing prices based on demographic and geographic features such as median income, housing age, and population density. You'll begin by exploring the dataset, which is crucial for any type of data based modeling, and then fit models with different types of tradeoffs.\n",
    "\n",
    "## 6a. Exploring the Dataset\n",
    "\n",
    "Before modeling, it's important to understand what kind of data you're working with. Even though you do not strictly need to do so to answer the question, consider using visualizations and summary statistics to explore relationships between variables and prices. Are any features clearly predictive? Are there outliers or skewed distributions? Consider how these factors might affect modeling later on. Building intuiting on the data you are modelling pretty much always pays off, especially when trying to identify and fix modelling problems and bugs.\n",
    "\n",
    "Report the following:\n",
    "- Number of samples in the dataset:\n",
    "- Number of features in the dataset, excluding the target:\n",
    "\n",
    "Optional task (not graded):\n",
    "- Plot the relationship between the target (price) and the different features.\n",
    "\n",
    "## 6b. Polynomial Feature Expansion\n",
    "\n",
    "Linear regression can only capture straight-line relationships — but housing data often involves more complexity. To capture nonlinear patterns, you’ll expand the input features using a polynomial of various degrees, which includes squares and interaction terms between features. Use `PolynomialFeatures` from `sklearn.preprocessing` to construct this expanded design matrix.\n",
    "\n",
    "However, polynomial features can lead to numerical instability, especially when the original features vary in scale. Large feature values produce large squared terms, which can cause issues during optimization. In practice, this results in warnings about ill-conditioned matrices. To avoid this, you’ll standardize the original data using `StandardScaler` before generating polynomial features.\n",
    "\n",
    "Once the transformation is complete, report the shape of the polynomial design matrix, after expansion with a polynomial of degree 2:\n",
    "\n",
    "**Do not include bias when constructing the features using PolynomialFeatures**.\n",
    "\n",
    "These observations should help you develop intuition for the cost of model complexity.\n",
    "\n",
    "## 6c. Fitting A Regression\n",
    "With your polynomial design matrix in hand, you'll now compute the regression model that minimizes the residual sum of squares (RSS) and compare it to the performance of a linear model. \n",
    "\n",
    "**Use the data set split in the accompanying notebook to answer the following questions**. You will have a training and validation dataset. Only use the training dataset to fit the models.  \n",
    "\n",
    "After solving for the regression parameters, report the following parameters for each of two models, and report the mean squared error (MSE) on the validation set:\n",
    "\n",
    "Linear model:\n",
    "- $ \\beta_{\\text{MedInc}} $\n",
    "- $ \\beta_{\\text{AveBedrms}} $\n",
    "- $ \\beta_{\\text{HouseAge}} $\n",
    "- $MSE_{val} = $ \n",
    "\n",
    "Polynomial of degree 2:\n",
    "- $ \\beta_{\\text{MedInc}} $\n",
    "- $ \\beta_{\\text{MedInc} \\cdot \\text{AveBedrms}} $\n",
    "- $ \\beta_{\\text{HouseAge} \\cdot \\text{AveBedrms}} $\n",
    "- $MSE = $\n",
    "\n",
    "Optional (not graded): Plot data sample of your trained models on top of the data to get a sense of the model fit for different target-feature combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183cfa1dc9c5c42a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 7. Regularization And Cross Validation\n",
    "\n",
    "Building on your polynomial regression models from Exercise 6, you'll now explore two critical concepts in machine learning: regularization to prevent overfitting, and cross-validation to obtain more reliable performance estimates. These techniques address fundamental challenges that arise when working with complex models on real data.\n",
    "In Exercise 6, you likely observed that the polynomial model achieved lower training error than the linear model, but this doesn't necessarily mean it will generalize better to new data. Exercise 7 introduces tools to address this challenge systematically.\n",
    "\n",
    "## 7a. Regularization\n",
    "\n",
    "To counter overfitting and improve stability, you’ll re-fit the models using **Ridge regression**, which penalizes large weights via an L2 penalty. The specific objective here is a _modified version_ of the standard ridge regression objective function, with $n$ indicating the number of data points used to fit the model:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\frac{1}{n} \\| y - X\\beta \\|^2 + \\lambda \\| \\beta \\|^2\n",
    "$$\n",
    "\n",
    "Implement ridge regression using the same linear and polynomial design matrix, and set $ \\lambda = 0.001 $. As before, examine how the regularization changes the learned parameters and the mean squared error (MSE).\n",
    "\n",
    "Report the same three coefficients:\n",
    "\n",
    "Linear model:\n",
    "- $ \\beta_{\\text{MedInc}} $\n",
    "- $ \\beta_{\\text{AveBedrms}} $\n",
    "- $ \\beta_{\\text{HouseAge}} $\n",
    "- $MSE_{val} = $ \n",
    "\n",
    "Polynomial of degree 2:\n",
    "- $ \\beta_{\\text{MedInc}} =$\n",
    "- $ \\beta_{\\text{MedInc} \\cdot \\text{AveBedrms}} =$\n",
    "- $ \\beta_{\\text{HouseAge} \\cdot \\text{AveBedrms}} =$\n",
    "- $MSE_{val} = $\n",
    "\n",
    "## 7b. Cross-Validation\n",
    "\n",
    "Cross-validation provides a more robust estimate of model performance than a single train-validation split, at the cost of significantly more computation. By systematically training and evaluating on different subsets of the data, you can better understand how well your models generalize and reduce the risk of making decisions based on a particular \"lucky\" or \"unlucky\" split.\n",
    "\n",
    "**Use the 5-fold cross-validation splits provided in the setup notebook to answer the following question.** For each fold, you'll train your models on four folds and evaluate on the fifth, repeating this process for all five folds. This gives you five performance estimates that you can aggregate to get a more reliable assessment of model quality.\n",
    "\n",
    "You'll evaluate both the linear model and the polynomial degree 2 model using the same Ridge regression approach from 7a. Remember to apply the same preprocessing pipeline within each fold: standardize the training folds, apply polynomial expansion if needed, then fit the ridge regression model.\n",
    "\n",
    "**Important**: Each fold should be treated as an independent experiment. This means you should standardize features using only the training folds for that iteration, not the entire dataset. This prevents data leakage and ensures your cross-validation estimates are unbiased.\n",
    "\n",
    "After completing cross-validation, also evaluate both models on the held-out validation set (`X_val`, `y_val`) to assess final performance on truly unseen data.\n",
    "\n",
    "Report the following metrics:\n",
    "\n",
    "**Linear model (Ridge):**\n",
    "- Mean Cross-Validation MSE = \n",
    "- Standard deviation of Cross-Validation MSE = \n",
    "- Final validation MSE = \n",
    "\n",
    "**Polynomial degree 2 model (Ridge):**\n",
    "- Mean Cross-Validation MSE = \n",
    "- Standard deviation of Cross-Validation MSE = \n",
    "- Final validation MSE = \n",
    "\n",
    "**Optional task (not graded):** How do the cross-validation estimates compare to the final validation performance? What does this tell you about the reliability of your model selection process? How do the measures compare to a single train-test split? Did regularization help performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c67c3b0-9363-494a-981c-74055fa6905d",
   "metadata": {},
   "source": [
    "# 8. California Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fc600",
   "metadata": {},
   "source": [
    "This open question will test your understanding of the entire modeling process. Use the California housing dataset (see the setup notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d5a57",
   "metadata": {},
   "source": [
    "## 8a. Feature Selection\n",
    "\n",
    "What are the candidate features of this dataset that could be removed according to variance thresholding or correlation-based feature selection? Explain how you derive the candidates by means of plots or tables and choose at least three features that are candidates for removal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fc9118",
   "metadata": {},
   "source": [
    "## 8b. Cross Validation\n",
    "\n",
    "Implement a function that returns the regression model for a given design matrix and target vector. Fit and evaluate four regression models (with affine basis functions) using (I) all features and (II-IV) all but one of the three candidate features from the previous part. Use 5-fold cross-validation to evaluate your regression models. Add a table in the report with the cross-validated scores for each of your regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3274447",
   "metadata": {},
   "source": [
    "## 8c. Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158e15cd",
   "metadata": {},
   "source": [
    "Interpret your results. Would you recommend removing one of the candidate features? What would you infer from the cross-validated scores?\n",
    "\n",
    "Justify your analysis and discuss possible benefits and drawbacks when removing one of the features vs. keeping all features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
